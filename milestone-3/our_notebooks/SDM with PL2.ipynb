{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c3da078-f7fc-4d37-904c-532bb26d4321",
   "metadata": {},
   "source": [
    "# SDM With PL2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66fd2911-c97a-4f91-af28-8c7e381573b6",
   "metadata": {},
   "source": [
    "### Step 1: Import everything and load variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ae3c54f-aba1-45bf-b074-e78a99f6405f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start PyTerrier with version=5.7, helper_version=0.0.7, no_download=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.9.2 has loaded Terrier 5.7 (built by craigm on 2022-11-10 18:30) and terrier-helper 0.0.7\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will use a small hardcoded example located in ./iranthology-dataset-tira.\n",
      "The output directory is /tmp/\n"
     ]
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "import pandas as pd\n",
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, get_input_directory_and_output_directory, persist_and_normalize_run\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "ensure_pyterrier_is_loaded()\n",
    "input_directory, output_directory = get_input_directory_and_output_directory('./iranthology-dataset-tira')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c563b0e-97ac-44a2-ba2f-18858f1506bb",
   "metadata": {},
   "source": [
    "### Step 2: Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e35230af-66ec-4607-a97b-127bd890fa59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Load the data.\n",
      "We look at the first document:\n",
      "\n",
      "{'docno': '2019.sigirconf_workshop-2019birndl.0', 'text': 'Proceedings of the 4th Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2019) co-located with the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2019), Paris, France, July 25, 2019 ', 'title': 'Proceedings of the 4th Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2019) co-located with the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2019), Paris, France, July 25, 2019', 'abstract': ''}\n"
     ]
    }
   ],
   "source": [
    "queries = pt.io.read_topics(input_directory + '/milestone-1-topics.xml', format='trecxml')\n",
    "\n",
    "#qrels = open(input_directory + '/qrels.txt', \"r\")\n",
    "\n",
    "documents = [json.loads(i) for i in open(input_directory + '/documents.jsonl', 'r')]\n",
    "documents = [{'docno': i['docno'], 'text': i['text'], 'title': i['original_document']['title'], 'abstract': i['original_document']['abstract']} for i in documents]\n",
    "\n",
    "print('We look at the first document:\\n')\n",
    "print(documents[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72655916-07fe-4c58-82c1-2f9f93381e7f",
   "metadata": {},
   "source": [
    "### Step 3: Create the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05ce062d-25e4-4c61-b6ce-9431b9f2bbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Create the Index.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|█████████████▌                              | 16558/53673 [00:35<00:18, 1984.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:54:02.329 [ForkJoinPool-1-worker-3] WARN org.terrier.structures.indexing.Indexer - Adding an empty document to the index (2020.mir_conference-2020.1) - further warnings are suppressed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 53673/53673 [01:06<00:00, 811.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:54:57.297 [ForkJoinPool-1-worker-3] WARN org.terrier.structures.indexing.Indexer - Indexed 3 empty documents\n"
     ]
    }
   ],
   "source": [
    "print('Step 3: Create the Index.')\n",
    "\n",
    "!rm -Rf ./index\n",
    "iter_indexer = pt.IterDictIndexer(\"./index\", meta={'docno' : 100, 'title': 10240, 'abstract': 10240, 'text': 10240}, blocks=True)\n",
    "index_ref = iter_indexer.index(tqdm(documents))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23f2010b",
   "metadata": {},
   "source": [
    "### PL2\n",
    "\n",
    "Pl2 is a probabilistic scoring metric. It ranks the documents based the term occurence and distribution on how likely they're related to a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7505472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl2 = pt.BatchRetrieve(index_ref, wmodel=\"PL2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "806c4638-ccee-4470-a74c-2a85d9ee2cfc",
   "metadata": {},
   "source": [
    "### Step 4: Create Retrieval Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6104053d",
   "metadata": {},
   "source": [
    "Originally we wanted to try neural scoring systems like Bert out, which would have a lot more context-based approach to ranking. The notebook + docker kinda restricted us, which is the reason why we decided against it. SDM (Sequential Dependence Model) expands the query by connecting words that appear together / in the same context, which is the reason why we thought it could be more accurate. We try SDM query expansion together with different scoring systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "642259bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdm = pt.rewrite.SequentialDependence()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93fc2136",
   "metadata": {},
   "source": [
    "The query is first expandend with sdm, then ranked with the pl2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61882305",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_pipeline = sdm >> pl2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5cb6607b",
   "metadata": {},
   "source": [
    "### Step 5: Create the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a191f396-e896-4792-afaf-574e452640f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Create Run.\n"
     ]
    }
   ],
   "source": [
    "print('Step 5: Create Run.')\n",
    "\n",
    "run = retrieval_pipeline(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c0e07fca-de98-4de2-b6a7-abfd516c652c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We look at the first 10 results of the run (query has ben expanded):\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query</th>\n",
       "      <th>query_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14568</td>\n",
       "      <td>2019.wsdm_conference-2019.40</td>\n",
       "      <td>0</td>\n",
       "      <td>15.375928</td>\n",
       "      <td>relevant documents show bigram fake news synon...</td>\n",
       "      <td>relevant documents show the bigram of fake ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>33491</td>\n",
       "      <td>2018.wwwconf_conference-2018c.140</td>\n",
       "      <td>1</td>\n",
       "      <td>14.010381</td>\n",
       "      <td>relevant documents show bigram fake news synon...</td>\n",
       "      <td>relevant documents show the bigram of fake ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>21754</td>\n",
       "      <td>2020.cikm_conference-2020.118</td>\n",
       "      <td>2</td>\n",
       "      <td>13.814559</td>\n",
       "      <td>relevant documents show bigram fake news synon...</td>\n",
       "      <td>relevant documents show the bigram of fake ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14643</td>\n",
       "      <td>2019.wsdm_conference-2019.115</td>\n",
       "      <td>3</td>\n",
       "      <td>13.800234</td>\n",
       "      <td>relevant documents show bigram fake news synon...</td>\n",
       "      <td>relevant documents show the bigram of fake ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>7566</td>\n",
       "      <td>2008.sigirconf_conference-2008.116</td>\n",
       "      <td>4</td>\n",
       "      <td>13.045042</td>\n",
       "      <td>relevant documents show bigram fake news synon...</td>\n",
       "      <td>relevant documents show the bigram of fake ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>21379</td>\n",
       "      <td>2009.cikm_conference-2009.191</td>\n",
       "      <td>5</td>\n",
       "      <td>12.202793</td>\n",
       "      <td>relevant documents show bigram fake news synon...</td>\n",
       "      <td>relevant documents show the bigram of fake ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>30099</td>\n",
       "      <td>2019.wwwconf_conference-2019c.232</td>\n",
       "      <td>6</td>\n",
       "      <td>12.099965</td>\n",
       "      <td>relevant documents show bigram fake news synon...</td>\n",
       "      <td>relevant documents show the bigram of fake ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>17492</td>\n",
       "      <td>2021.ecir_conference-20212.9</td>\n",
       "      <td>7</td>\n",
       "      <td>11.895309</td>\n",
       "      <td>relevant documents show bigram fake news synon...</td>\n",
       "      <td>relevant documents show the bigram of fake ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>13000</td>\n",
       "      <td>2020.clef_conference-2020w.52</td>\n",
       "      <td>8</td>\n",
       "      <td>11.771914</td>\n",
       "      <td>relevant documents show bigram fake news synon...</td>\n",
       "      <td>relevant documents show the bigram of fake ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>14162</td>\n",
       "      <td>2020.fire_conference-2020w.66</td>\n",
       "      <td>9</td>\n",
       "      <td>11.751446</td>\n",
       "      <td>relevant documents show bigram fake news synon...</td>\n",
       "      <td>relevant documents show the bigram of fake ne...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid  docid                               docno  rank      score  \\\n",
       "0   1  14568        2019.wsdm_conference-2019.40     0  15.375928   \n",
       "1   1  33491   2018.wwwconf_conference-2018c.140     1  14.010381   \n",
       "2   1  21754       2020.cikm_conference-2020.118     2  13.814559   \n",
       "3   1  14643       2019.wsdm_conference-2019.115     3  13.800234   \n",
       "4   1   7566  2008.sigirconf_conference-2008.116     4  13.045042   \n",
       "5   1  21379       2009.cikm_conference-2009.191     5  12.202793   \n",
       "6   1  30099   2019.wwwconf_conference-2019c.232     6  12.099965   \n",
       "7   1  17492        2021.ecir_conference-20212.9     7  11.895309   \n",
       "8   1  13000       2020.clef_conference-2020w.52     8  11.771914   \n",
       "9   1  14162       2020.fire_conference-2020w.66     9  11.751446   \n",
       "\n",
       "                                               query  \\\n",
       "0  relevant documents show bigram fake news synon...   \n",
       "1  relevant documents show bigram fake news synon...   \n",
       "2  relevant documents show bigram fake news synon...   \n",
       "3  relevant documents show bigram fake news synon...   \n",
       "4  relevant documents show bigram fake news synon...   \n",
       "5  relevant documents show bigram fake news synon...   \n",
       "6  relevant documents show bigram fake news synon...   \n",
       "7  relevant documents show bigram fake news synon...   \n",
       "8  relevant documents show bigram fake news synon...   \n",
       "9  relevant documents show bigram fake news synon...   \n",
       "\n",
       "                                             query_0  \n",
       "0   relevant documents show the bigram of fake ne...  \n",
       "1   relevant documents show the bigram of fake ne...  \n",
       "2   relevant documents show the bigram of fake ne...  \n",
       "3   relevant documents show the bigram of fake ne...  \n",
       "4   relevant documents show the bigram of fake ne...  \n",
       "5   relevant documents show the bigram of fake ne...  \n",
       "6   relevant documents show the bigram of fake ne...  \n",
       "7   relevant documents show the bigram of fake ne...  \n",
       "8   relevant documents show the bigram of fake ne...  \n",
       "9   relevant documents show the bigram of fake ne...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('We look at the first 10 results of the run (query has ben expanded):\\n')\n",
    "run.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28c40a2e-0f96-4ae8-aa5e-55a5e7ef9dee",
   "metadata": {},
   "source": [
    "### Step 6: Persist Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "12e5bb42-ed1f-41ba-b7a5-cb43ebca96f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6: Persist Run.\n"
     ]
    }
   ],
   "source": [
    "print('Step 6: Persist Run.')\n",
    "\n",
    "persist_and_normalize_run(run, output_file=output_directory, system_name='multi-field', depth=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c62a055d",
   "metadata": {},
   "source": [
    "### Self-Reflection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8247da6f",
   "metadata": {},
   "source": [
    "#### Iris\n",
    "\n",
    "A lot of time of the Milestone 3 was I used for trying to configure docker and the things that I needed to be able to work with pyterrier. In the end everything worked out, but I used an awful lot of time, which left less time for the actual pyterrier. It was difficult to figure out ways to rank the documents, especially because there wasn't really much information except the pyterrier documentation. I still found it a lot of trying out different approaches and learning about them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0cf1dd4a",
   "metadata": {},
   "source": [
    "### Justin\n",
    "The tasks for milestone 3 were a bit more fun for me because we could try different approaches to find a better retrieval pipeline. Unfortunality we had problems with understanding the tutorial and which dataset we should use and whether we have to register it in TIRA. Then we thought we can evaluate our approaches directly in TIRA but we wondered why we get no results instantly. As we know that the evaluating made manually we could concentrate on find approaches. But we didn't find out how we can evaluate our approaches correctly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
