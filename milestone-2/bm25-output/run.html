<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <link rel="icon"
    href="https://raw.githubusercontent.com/capreolus-ir/diffir/master/docs/images/icon.png">
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
    integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-select@1.13.14/dist/css/bootstrap-select.min.css">
  <title>diffir: IR model comparision</title>
  <style>
    .card {
      margin: 5px !important;
    }

    .highlight {
      background-color: #ffffd3;
    }

    #DocumentOverlay {
      position: fixed;
      top: 0;
      bottom: 0;
      left: 0;
      right: 0;
      background-color: rgba(0, 0, 0, .25);
    }

    #DocumentDetails {
      position: fixed;
      top: 60px;
      left: 10%;
      right: 10%;
      bottom: 60px;
      background-color: white;
      padding: 20px;
      border: 1px solid rgba(0, 0, 0, .125);
      border-radius: 0.25rem;
      box-shadow: 0 0 16px black;
      overflow: auto;
    }

    .close-overlay {
      position: absolute;
      top: 4px;
      right: 4px;
      border-radius: 100%;
      background-color: #111;
      font-size: 17px;
      padding: 9px;
      color: white;
      width: 30px;
      height: 30px;
      text-align: center;
      font-weight: normal;
      line-height: 11px;
      cursor: pointer;
    }

    .docid {
      background-color: rgb(224, 135, 55);
      position: absolute;
      top: 0;
      bottom: 0;
      width: 20px;
      overflow: hidden;
      margin-bottom: 0;
      border-radius: 0;
      font-weight: normal;
      white-space: nowrap;
    }

    .docid-value {
      transform: rotate(90deg);
      font-size: 0.7em;
      padding-left: 8px;
    }

    .fields th {
      vertical-align: top;
      text-align: right;
      padding-right: 12px;
      color: #999;
      font-weight: normal;
    }

    #query-container {
      max-width: 600px;
      border: 1px solid #999;
      border-radius: 0.25rem;
      margin: 20px auto;
      padding: 10px;
    }

    .other-rank {
      font-size: 1.2em;
      display: inline-block;
      width: 20px;
      margin-top: 46px;
      margin-left: 3px;
      margin-right: 3px;
      cursor: help;
    }

    mark {
      padding: 0;
      font-weight: bold;
    }

    .snippet {
      font-size: 0.9em;
      line-height: 1.2;
    }

    .elip {
      text-align: center;
      margin: 16px;
      color: gray;
    }

    .doc-info {
      white-space: nowrap;
    }

    .card-header {
      min-height: 128px;
      cursor: pointer;
    }

    .swatch {
      display: inline-block;
      width: 16px;
      height: 16px;
      vertical-align: middle;
    }

    .form-group {
      width: 150px;
      height: 20px;
      padding-left: 10px;
      padding-top: 10px;
    }

    .nobackground {
      background: transparent !important;
      font-weight: normal;
    }
    .styled-table {
      margin-left: 0px;
      margin-top: 10px;
      border-collapse: collapse;    
      font-size: 0.9em;
      /* font-family: sans-serif;       */
      min-width: 350px;      
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.15);
    }
    .styled-table thead tr {
      background-color: #17a2b8;
      color: #ffffff;
      text-align: left;
    }    
    /* .styled-table th, */
    .styled-table td {
      /* padding: 12px 15px; */
      text-align: center;
    }    
    .styled-table tbody tr {
      color: #ffffff;
    }
    /*
    .styled-table tbody tr:nth-of-type(even) {
      background-color: #f3f3f3;
    } */

    .styled-table tbody tr:last-of-type {
      border-bottom: 2px solid #009879;
    }

    #ranking-summary ul li span {
      margin-right: 5px;
    }    
  </style>
</head>

<body>
  <header>
    <div class="collapse bg-dark" id="navbarHeader">
      <div class="container">
        <div class="row">
          <div class="col-sm-6 col-md-6 py-4">
            <h6 class="text-white">Summary</h6>
            <p class="text-white" id="ranking-summary"></p>
          </div>
          <div class="col-sm-5 col-md-5 py-4"> 
            <h6 class="text-white">Ranking statistics</h6>                                   
            <ul>
              <li class="text-white"><span id="contrast-measure"></span></li>
              <li class="text-white"> <span>Relevance metrics</span> <div id="metrics"></div></li>
            </ul>                        
          </div>
        </div>
      </div>
    </div>
    <div class="navbar navbar-dark bg-dark box-shadow navbar-fixed-top">
      <div class="container d-flex justify-content-between">
        <a href="#" class="navbar-brand d-flex align-items-center">
          <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-search"
            viewBox="0 0 16 16">
            <path
              d="M11.742 10.344a6.5 6.5 0 1 0-1.397 1.398h-.001c.03.04.062.078.098.115l3.85 3.85a1 1 0 0 0 1.415-1.414l-3.85-3.85a1.007 1.007 0 0 0-.115-.1zM12 6.5a5.5 5.5 0 1 1-11 0 5.5 5.5 0 0 1 11 0z" />
          </svg>
          <strong style="padding-left:  5px;">DiffIR</strong>
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarHeader"
          aria-controls="navbarHeader" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
      </div>
    </div>
  </header>
  <div class="container">
    <div class="input-group" style="padding-top: 50px; padding-bottom: 10px; background-color: white;">
      <select id="Queries" data-width="100%" data-style="border" data-container="body"></select>
    </div>
    <div id="query-container" class="sticky-top" style="background-color: white;">
      <div style="position:relative">
        <button id="query-collapse-btn" style="position: absolute; top: 12px; right: 8px;" type="button"
          class="btn btn-outline-info btn-sm" data-toggle="collapse" data-target=".query_collapse" aria-expanded="false"
          aria-controls="query_collapse">
          <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor"
            class="bi bi-arrows-angle-expand" viewBox="0 0 16 16">
            <path fill-rule="evenodd"
              d="M5.828 10.172a.5.5 0 0 0-.707 0l-4.096 4.096V11.5a.5.5 0 0 0-1 0v3.975a.5.5 0 0 0 .5.5H4.5a.5.5 0 0 0 0-1H1.732l4.096-4.096a.5.5 0 0 0 0-.707zm4.344-4.344a.5.5 0 0 0 .707 0l4.096-4.096V4.5a.5.5 0 1 0 1 0V.525a.5.5 0 0 0-.5-.5H11.5a.5.5 0 0 0 0 1h2.768l-4.096 4.096a.5.5 0 0 0 0 .707z" />
          </svg>
        </button>
      </div>
      <div id="Query" style="padding-right: 45px;">
      </div>
    </div>
    <div class="row justify-content-center" id="runName">
      <div class="col">
        <h6 id="Run1Name" style="text-align: center;"></h6>
      </div>
      <div class="col">
        <h6 id="Run2Name" style="text-align: center;"></h6>
      </div>
    </div>
    <div class="row" id="docList">
      <div id="Run1Docs" class="col">
      </div>
      <div id="Run2Docs" class="col">
      </div>
    </div>    
  </div>
  <!-- Optional JavaScript -->
  <!-- jQuery first, then Popper.js, then Bootstrap JS -->
  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
    integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
    crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"
    integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1"
    crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"
    integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM"
    crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap-select@1.13.14/dist/js/bootstrap-select.min.js"></script>
  <script type="text/javascript">
    var data = {"meta": {"run1_name": "/tira-data/output/run.txt", "run2_name": null, "dataset": "iranthology-ir-lab-sose2023-information-retrievers", "measure": "topk", "qrelDefs": {"0": "Not Relevant", "1": "Relevant"}, "queryFields": ["query_id", "title", "description", "narrative"], "docFields": ["doc_id", "text"], "relevanceColors": {"null": "#888888", "0": "#d54541", "1": "#52b262"}}, "queries": [{"fields": {"query_id": "1", "title": "fake news detection", "description": "All publications and workshops concerning the detection (identification) of fake news (disinformation, misinformation or deep fakes)", "narrative": "Relevant documents show the bigram of \"fake news\" or its synonyms in the context of their detection mechanisms.", "contrast": {"name": "singlerun", "value": 0}}, "metrics": {"P@1": [1.0], "P@3": [1.0], "P@5": [1.0], "P@10": [0.9], "nDCG@1": [1.0], "nDCG@3": [1.0], "nDCG@5": [1.0], "nDCG@10": [0.9216017310213247]}, "run_1": [{"doc_id": "2019.wsdm_conference-2019.115", "score": 27.200500874593587, "relevance": 1, "rank": 1, "weights": {"doc_id": [], "default_text": [[476, 480, 1.0], [481, 485, 1.0], [567, 571, 1.0], [572, 576, 1.0], [577, 586, 1.0], [632, 636, 1.0], [637, 641, 1.0], [860, 864, 1.0], [865, 869, 1.0], [956, 960, 1.0], [961, 965, 1.0], [1009, 1013, 1.0], [1014, 1018, 1.0], [1095, 1099, 1.0], [1100, 1104, 1.0], [1271, 1280, 1.0], [1336, 1340, 1.0], [1341, 1345, 1.0], [1346, 1355, 1.0], [1429, 1433, 1.0], [1434, 1438, 1.0], [1439, 1448, 1.0], [1490, 1494, 1.0], [1775, 1779, 1.0], [1780, 1784, 1.0], [1785, 1794, 1.0], [1832, 1836, 1.0], [1837, 1841, 1.0], [1865, 1874, 1.0]]}, "snippet": {"field": "default_text", "start": 471, "stop": 671, "weights": [[5, 9, 1.0], [10, 14, 1.0], [96, 100, 1.0], [101, 105, 1.0], [106, 115, 1.0], [161, 165, 1.0], [166, 170, 1.0]]}}, {"doc_id": "2018.wwwconf_conference-2018c.140", "score": 26.183818538040047, "relevance": 1, "rank": 2, "weights": {"doc_id": [], "default_text": [[536, 540, 1.0], [541, 545, 1.0], [630, 634, 1.0], [635, 639, 1.0], [710, 714, 1.0], [826, 830, 1.0], [845, 849, 1.0], [970, 974, 1.0], [975, 979, 1.0], [1076, 1080, 1.0], [1081, 1085, 1.0], [1334, 1338, 1.0], [1339, 1343, 1.0], [1492, 1496, 1.0], [1576, 1580, 1.0], [1800, 1804, 1.0], [1805, 1809, 1.0], [1810, 1819, 1.0], [1821, 1825, 1.0], [1826, 1830, 1.0], [1831, 1840, 1.0]]}, "snippet": {"field": "default_text", "start": 1795, "stop": 1995, "weights": [[5, 9, 1.0], [10, 14, 1.0], [15, 24, 1.0], [26, 30, 1.0], [31, 35, 1.0], [36, 45, 1.0]]}}, {"doc_id": "2020.clef_conference-2020w.200", "score": 24.150445777089644, "relevance": 1, "rank": 3, "weights": {"doc_id": [], "default_text": [[655, 659, 1.0], [732, 736, 1.0], [882, 886, 1.0], [900, 904, 1.0], [905, 909, 1.0], [928, 932, 1.0], [933, 937, 1.0], [987, 991, 1.0], [992, 996, 1.0], [1308, 1312, 1.0], [1313, 1317, 1.0], [1517, 1521, 1.0], [1522, 1526, 1.0], [1582, 1586, 1.0], [1587, 1591, 1.0], [1635, 1639, 1.0], [1640, 1644, 1.0], [1689, 1693, 1.0], [1694, 1698, 1.0]]}, "snippet": {"field": "default_text", "start": 1512, "stop": 1712, "weights": [[5, 9, 1.0], [10, 14, 1.0], [70, 74, 1.0], [75, 79, 1.0], [123, 127, 1.0], [128, 132, 1.0], [177, 181, 1.0], [182, 186, 1.0]]}}, {"doc_id": "2020.clef_conference-2020w.52", "score": 23.678194461234746, "relevance": 1, "rank": 4, "weights": {"doc_id": [], "default_text": [[536, 540, 1.0], [541, 545, 1.0], [861, 865, 1.0], [866, 870, 1.0], [977, 981, 1.0], [982, 986, 1.0], [996, 1005, 1.0], [1022, 1026, 1.0], [1027, 1031, 1.0]]}, "snippet": {"field": "default_text", "start": 856, "stop": 1056, "weights": [[5, 9, 1.0], [10, 14, 1.0], [121, 125, 1.0], [126, 130, 1.0], [140, 149, 1.0], [166, 170, 1.0], [171, 175, 1.0]]}}, {"doc_id": "2020.clef_conference-2020w.111", "score": 23.382960775754896, "relevance": 1, "rank": 5, "weights": {"doc_id": [], "default_text": [[469, 473, 1.0], [474, 478, 1.0], [512, 516, 1.0], [545, 554, 1.0], [558, 562, 1.0], [563, 567, 1.0], [741, 745, 1.0], [746, 750, 1.0], [821, 825, 1.0], [826, 830, 1.0], [1466, 1470, 1.0], [1471, 1475, 1.0], [1500, 1509, 1.0], [1513, 1517, 1.0], [1518, 1522, 1.0]]}, "snippet": {"field": "default_text", "start": 464, "stop": 664, "weights": [[5, 9, 1.0], [10, 14, 1.0], [48, 52, 1.0], [81, 90, 1.0], [94, 98, 1.0], [99, 103, 1.0]]}}, {"doc_id": "2018.sigirconf_conference-2018.30", "score": 23.355508121128324, "relevance": 0, "rank": 6, "weights": {"doc_id": [], "default_text": [[499, 503, 1.0], [504, 508, 1.0], [567, 571, 1.0], [572, 576, 1.0], [638, 642, 1.0], [643, 647, 1.0], [763, 767, 1.0], [768, 772, 1.0], [1014, 1018, 1.0], [1019, 1023, 1.0], [1605, 1609, 1.0], [1610, 1614, 1.0]]}, "snippet": {"field": "default_text", "start": 494, "stop": 694, "weights": [[5, 9, 1.0], [10, 14, 1.0], [73, 77, 1.0], [78, 82, 1.0], [144, 148, 1.0], [149, 153, 1.0]]}}, {"doc_id": "2020.clef_conference-2020w.74", "score": 23.279387142055388, "relevance": 1, "rank": 7, "weights": {"doc_id": [], "default_text": [[464, 468, 1.0], [469, 473, 1.0], [504, 508, 1.0], [622, 626, 1.0], [627, 631, 1.0], [944, 948, 1.0], [1090, 1094, 1.0], [1095, 1099, 1.0], [1213, 1217, 1.0], [1218, 1222, 1.0], [1334, 1338, 1.0], [1339, 1343, 1.0], [1451, 1455, 1.0], [1456, 1460, 1.0], [1461, 1470, 1.0], [1983, 1987, 1.0], [1988, 1992, 1.0], [2141, 2145, 1.0], [2146, 2150, 1.0], [2160, 2169, 1.0]]}, "snippet": {"field": "default_text", "start": 459, "stop": 659, "weights": [[5, 9, 1.0], [10, 14, 1.0], [45, 49, 1.0], [163, 167, 1.0], [168, 172, 1.0]]}}, {"doc_id": "2020.clef_conference-2020w.24", "score": 23.10048569908956, "relevance": 1, "rank": 8, "weights": {"doc_id": [], "default_text": [[582, 586, 1.0], [587, 591, 1.0], [716, 725, 1.0], [729, 733, 1.0], [734, 738, 1.0], [958, 962, 1.0], [1323, 1327, 1.0], [1328, 1332, 1.0], [1409, 1418, 1.0], [1422, 1426, 1.0]]}, "snippet": {"field": "default_text", "start": 577, "stop": 777, "weights": [[5, 9, 1.0], [10, 14, 1.0], [139, 148, 1.0], [152, 156, 1.0], [157, 161, 1.0]]}}, {"doc_id": "2020.clef_conference-2020w.54", "score": 22.934616842633815, "relevance": 1, "rank": 9, "weights": {"doc_id": [], "default_text": [[536, 540, 1.0], [541, 545, 1.0], [957, 961, 1.0], [962, 966, 1.0], [1079, 1083, 1.0], [1084, 1088, 1.0], [1487, 1491, 1.0], [1492, 1496, 1.0], [1747, 1751, 1.0], [1752, 1756, 1.0]]}, "snippet": {"field": "default_text", "start": 952, "stop": 1152, "weights": [[5, 9, 1.0], [10, 14, 1.0], [127, 131, 1.0], [132, 136, 1.0]]}}, {"doc_id": "2020.clef_conference-2020w.169", "score": 22.79046827553917, "relevance": 1, "rank": 10, "weights": {"doc_id": [], "default_text": [[624, 628, 1.0], [629, 633, 1.0], [683, 687, 1.0], [688, 692, 1.0], [772, 776, 1.0], [777, 781, 1.0], [835, 839, 1.0], [840, 844, 1.0], [1030, 1034, 1.0], [1035, 1039, 1.0], [1147, 1151, 1.0], [1152, 1156, 1.0], [1342, 1346, 1.0]]}, "snippet": {"field": "default_text", "start": 619, "stop": 819, "weights": [[5, 9, 1.0], [10, 14, 1.0], [64, 68, 1.0], [69, 73, 1.0], [153, 157, 1.0], [158, 162, 1.0]]}}], "run_2": [], "summary": [], "mergedWeights": {}}, {"fields": {"query_id": "2", "title": " multimedia retrieval ", "description": " Which documents have information about multimedia information retrieval. ", "narrative": " To be relevant documents have to have methods and information of multimedia retrieval.", "contrast": {"name": "singlerun", "value": 0}}, "metrics": {"P@1": [1.0], "P@3": [1.0], "P@5": [1.0], "P@10": [0.7], "nDCG@1": [1.0], "nDCG@3": [1.0], "nDCG@5": [1.0], "nDCG@10": [0.7859162856301584]}, "run_1": [{"doc_id": "2008.ipm_journal-ir0anthology0volumeA44A1.20", "score": 9.482023641341247, "relevance": 1, "rank": 1, "weights": {"doc_id": [], "default_text": [[390, 400, 1.0], [525, 535, 1.0], [699, 709, 1.0], [737, 747, 1.0], [975, 985, 1.0], [1004, 1014, 1.0], [1129, 1139, 1.0], [1190, 1200, 1.0], [1229, 1239, 1.0], [1299, 1309, 1.0], [1310, 1319, 1.0], [1448, 1458, 1.0]]}, "snippet": {"field": "default_text", "start": 1124, "stop": 1324, "weights": [[5, 15, 1.0], [66, 76, 1.0], [105, 115, 1.0], [175, 185, 1.0], [186, 195, 1.0]]}}, {"doc_id": "2013.ipm_journal-ir0anthology0volumeA49A6.5", "score": 9.476196400995017, "relevance": 1, "rank": 2, "weights": {"doc_id": [], "default_text": [[682, 692, 1.0], [967, 976, 1.0], [1024, 1034, 1.0], [1035, 1044, 1.0], [1118, 1128, 1.0], [1178, 1188, 1.0], [1284, 1294, 1.0], [1373, 1383, 1.0], [1599, 1609, 1.0], [1686, 1696, 1.0], [1844, 1854, 1.0], [1855, 1864, 1.0], [2004, 2014, 1.0], [2024, 2033, 1.0]]}, "snippet": {"field": "default_text", "start": 962, "stop": 1162, "weights": [[5, 14, 1.0], [62, 72, 1.0], [73, 82, 1.0], [156, 166, 1.0]]}}, {"doc_id": "1997.cikm_workshop-97.16", "score": 9.390499760702262, "relevance": 1, "rank": 3, "weights": {"doc_id": [], "default_text": [[450, 460, 1.0], [473, 482, 1.0], [495, 505, 1.0], [621, 630, 1.0], [733, 743, 1.0], [825, 835, 1.0], [872, 882, 1.0], [1017, 1027, 1.0], [1040, 1049, 1.0], [1085, 1095, 1.0], [1246, 1255, 1.0]]}, "snippet": {"field": "default_text", "start": 445, "stop": 645, "weights": [[5, 15, 1.0], [28, 37, 1.0], [50, 60, 1.0], [176, 185, 1.0]]}}, {"doc_id": "2010.wwwconf_conference-2010.84", "score": 9.316103145195436, "relevance": 1, "rank": 4, "weights": {"doc_id": [], "default_text": [[443, 453, 1.0], [604, 613, 1.0], [924, 934, 1.0], [979, 989, 1.0], [1046, 1056, 1.0], [1149, 1159, 1.0], [1356, 1366, 1.0], [1461, 1471, 1.0], [1668, 1678, 1.0], [1721, 1731, 1.0], [1766, 1776, 1.0]]}, "snippet": {"field": "default_text", "start": 919, "stop": 1119, "weights": [[5, 15, 1.0], [60, 70, 1.0], [127, 137, 1.0]]}}, {"doc_id": "2009.wwwconf_workshop-2009ldow.10", "score": 9.227339569793577, "relevance": 1, "rank": 5, "weights": {"doc_id": [], "default_text": [[536, 546, 1.0], [621, 631, 1.0], [709, 719, 1.0], [781, 791, 1.0], [1031, 1041, 1.0], [1056, 1066, 1.0], [1107, 1117, 1.0]]}, "snippet": {"field": "default_text", "start": 531, "stop": 731, "weights": [[5, 15, 1.0], [90, 100, 1.0], [178, 188, 1.0]]}}, {"doc_id": "2012.sigirconf_conference-2012.50", "score": 9.201505445869671, "relevance": 0, "rank": 6, "weights": {"doc_id": [], "default_text": [[108, 117, 1.0], [453, 463, 1.0], [504, 514, 1.0], [695, 705, 1.0], [853, 863, 1.0], [1427, 1437, 1.0], [1525, 1535, 1.0], [1676, 1686, 1.0], [1714, 1724, 1.0], [1862, 1872, 1.0], [1924, 1934, 1.0]]}, "snippet": {"field": "default_text", "start": 1520, "stop": 1720, "weights": [[5, 15, 1.0], [156, 166, 1.0], [194, 204, 1.0]]}}, {"doc_id": "1986.tois_journal-ir0anthology0volumeA4A4.3", "score": 9.187163654637471, "relevance": 1, "rank": 7, "weights": {"doc_id": [], "default_text": [[393, 403, 1.0], [493, 503, 1.0], [541, 551, 1.0], [803, 813, 1.0], [1025, 1035, 1.0], [1095, 1105, 1.0], [1278, 1288, 1.0], [1396, 1406, 1.0], [1459, 1469, 1.0], [1696, 1706, 1.0]]}, "snippet": {"field": "default_text", "start": 388, "stop": 588, "weights": [[5, 15, 1.0], [105, 115, 1.0], [153, 163, 1.0]]}}, {"doc_id": "2013.sigirconf_conference-2013.1", "score": 9.180070747458636, "relevance": 0, "rank": 8, "weights": {"doc_id": [], "default_text": [[108, 117, 1.0], [553, 563, 1.0], [622, 632, 1.0], [709, 719, 1.0], [1505, 1515, 1.0], [1521, 1531, 1.0], [1614, 1624, 1.0], [1721, 1731, 1.0], [1960, 1970, 1.0], [2277, 2287, 1.0], [2315, 2324, 1.0], [2519, 2528, 1.0], [2532, 2542, 1.0], [3290, 3300, 1.0], [3555, 3565, 1.0], [3627, 3636, 1.0], [3680, 3690, 1.0], [3900, 3909, 1.0], [3925, 3935, 1.0], [3949, 3958, 1.0], [4036, 4045, 1.0], [4108, 4118, 1.0], [4175, 4185, 1.0], [4215, 4224, 1.0], [4237, 4247, 1.0]]}, "snippet": {"field": "default_text", "start": 3895, "stop": 4095, "weights": [[5, 14, 1.0], [30, 40, 1.0], [54, 63, 1.0], [141, 150, 1.0]]}}, {"doc_id": "1999.cikm_conference-99.67", "score": 9.078296314159083, "relevance": 0, "rank": 9, "weights": {"doc_id": [], "default_text": [[470, 480, 1.0], [524, 534, 1.0], [580, 589, 1.0], [622, 632, 1.0], [839, 849, 1.0], [1039, 1049, 1.0], [1205, 1215, 1.0], [1395, 1405, 1.0]]}, "snippet": {"field": "default_text", "start": 465, "stop": 665, "weights": [[5, 15, 1.0], [59, 69, 1.0], [115, 124, 1.0], [157, 167, 1.0]]}}, {"doc_id": "2007.ipm_journal-ir0anthology0volumeA43A5.17", "score": 9.04838821173417, "relevance": 1, "rank": 10, "weights": {"doc_id": [], "default_text": [[389, 399, 1.0], [725, 735, 1.0], [844, 854, 1.0], [965, 975, 1.0], [1191, 1201, 1.0], [1242, 1252, 1.0], [1524, 1534, 1.0], [2138, 2148, 1.0], [2162, 2171, 1.0], [2204, 2214, 1.0], [2446, 2456, 1.0], [2902, 2912, 1.0], [2962, 2972, 1.0], [3205, 3215, 1.0], [3237, 3246, 1.0], [3428, 3438, 1.0], [3571, 3581, 1.0], [3677, 3687, 1.0], [3837, 3847, 1.0]]}, "snippet": {"field": "default_text", "start": 2133, "stop": 2333, "weights": [[5, 15, 1.0], [29, 38, 1.0], [71, 81, 1.0]]}}], "run_2": [], "summary": [], "mergedWeights": {}}, {"fields": {"query_id": "3", "title": "processing natural language for information retrieval", "description": "Which documents contain informtion like problem definitions, methods, algorithms or challenges in processing natural language for information retrieval systems?", "narrative": "Relevant documents provide information about processing natural language associated with information retrieval. Documents containing information about information retrieval not associated with natural language or the other way round are not relevant.", "contrast": {"name": "singlerun", "value": 0}}, "metrics": {"P@1": [1.0], "P@3": [1.0], "P@5": [1.0], "P@10": [1.0], "nDCG@1": [1.0], "nDCG@3": [1.0], "nDCG@5": [1.0], "nDCG@10": [1.0]}, "run_1": [{"doc_id": "1985.tois_journal-ir0anthology0volumeA3A2.5", "score": 16.322250112876247, "relevance": 1, "rank": 1, "weights": {"doc_id": [], "default_text": [[355, 362, 1.0], [363, 371, 1.0], [372, 382, 1.0], [421, 428, 1.0], [429, 437, 1.0], [553, 563, 1.0], [888, 895, 1.0], [896, 904, 1.0], [905, 915, 1.0]]}, "snippet": {"field": "default_text", "start": 350, "stop": 550, "weights": [[5, 12, 1.0], [13, 21, 1.0], [22, 32, 1.0], [71, 78, 1.0], [79, 87, 1.0]]}}, {"doc_id": "2004.ipm_journal-ir0anthology0volumeA40A6.4", "score": 16.136418680556528, "relevance": 1, "rank": 2, "weights": {"doc_id": [], "default_text": [[467, 477, 1.0], [568, 575, 1.0], [576, 584, 1.0], [585, 595, 1.0], [682, 693, 1.0], [701, 708, 1.0], [709, 717, 1.0], [718, 728, 1.0], [802, 813, 1.0], [841, 848, 1.0], [849, 857, 1.0], [858, 868, 1.0], [1068, 1079, 1.0], [1087, 1094, 1.0], [1095, 1103, 1.0], [1104, 1114, 1.0], [1574, 1585, 1.0]]}, "snippet": {"field": "default_text", "start": 677, "stop": 877, "weights": [[5, 16, 1.0], [24, 31, 1.0], [32, 40, 1.0], [41, 51, 1.0], [125, 136, 1.0], [164, 171, 1.0], [172, 180, 1.0], [181, 191, 1.0]]}}, {"doc_id": "2020.wsdm_conference-2020.120", "score": 16.08944368511841, "relevance": 1, "rank": 3, "weights": {"doc_id": [], "default_text": [[452, 459, 1.0], [460, 468, 1.0], [469, 479, 1.0], [656, 663, 1.0], [664, 672, 1.0], [673, 683, 1.0], [758, 765, 1.0], [766, 774, 1.0], [775, 785, 1.0], [1229, 1236, 1.0], [1237, 1245, 1.0], [1246, 1256, 1.0]]}, "snippet": {"field": "default_text", "start": 651, "stop": 851, "weights": [[5, 12, 1.0], [13, 21, 1.0], [22, 32, 1.0], [107, 114, 1.0], [115, 123, 1.0], [124, 134, 1.0]]}}, {"doc_id": "2000.cikm_conference-2000.57", "score": 15.829842965828322, "relevance": 1, "rank": 4, "weights": {"doc_id": [], "default_text": [[81, 92, 1.0], [449, 456, 1.0], [457, 465, 1.0], [466, 476, 1.0], [508, 519, 1.0], [520, 529, 1.0], [802, 809, 1.0], [810, 818, 1.0], [943, 950, 1.0], [951, 959, 1.0], [1005, 1016, 1.0], [1150, 1159, 1.0], [1217, 1228, 1.0], [1229, 1238, 1.0], [1324, 1333, 1.0], [1371, 1378, 1.0], [1379, 1387, 1.0], [1388, 1398, 1.0]]}, "snippet": {"field": "default_text", "start": 1212, "stop": 1412, "weights": [[5, 16, 1.0], [17, 26, 1.0], [112, 121, 1.0], [159, 166, 1.0], [167, 175, 1.0], [176, 186, 1.0]]}}, {"doc_id": "1985.tois_journal-ir0anthology0volumeA3A2.1", "score": 15.662646208305908, "relevance": 1, "rank": 5, "weights": {"doc_id": [], "default_text": [[332, 339, 1.0], [340, 348, 1.0], [349, 359, 1.0], [410, 417, 1.0], [418, 426, 1.0], [427, 437, 1.0], [469, 479, 1.0], [598, 608, 1.0], [788, 798, 1.0], [853, 864, 1.0], [1079, 1086, 1.0], [1087, 1095, 1.0]]}, "snippet": {"field": "default_text", "start": 327, "stop": 527, "weights": [[5, 12, 1.0], [13, 21, 1.0], [22, 32, 1.0], [83, 90, 1.0], [91, 99, 1.0], [100, 110, 1.0], [142, 152, 1.0]]}}, {"doc_id": "2016.fire_conference-2016w.55", "score": 15.544515500415335, "relevance": 1, "rank": 6, "weights": {"doc_id": [], "default_text": [[60, 71, 1.0], [72, 81, 1.0], [854, 861, 1.0], [862, 870, 1.0], [871, 881, 1.0], [1430, 1437, 1.0], [1438, 1446, 1.0], [1447, 1457, 1.0], [1580, 1587, 1.0], [1588, 1596, 1.0], [1597, 1607, 1.0]]}, "snippet": {"field": "default_text", "start": 1425, "stop": 1625, "weights": [[5, 12, 1.0], [13, 21, 1.0], [22, 32, 1.0], [155, 162, 1.0], [163, 171, 1.0], [172, 182, 1.0]]}}, {"doc_id": "2005.clef_workshop-2005w.8", "score": 15.38871203440486, "relevance": 1, "rank": 7, "weights": {"doc_id": [], "default_text": [[625, 633, 1.0], [659, 669, 1.0], [680, 687, 1.0], [688, 696, 1.0], [697, 707, 1.0], [722, 731, 1.0], [797, 804, 1.0], [805, 813, 1.0], [814, 824, 1.0], [841, 849, 1.0], [850, 861, 1.0], [862, 871, 1.0]]}, "snippet": {"field": "default_text", "start": 654, "stop": 854, "weights": [[5, 15, 1.0], [26, 33, 1.0], [34, 42, 1.0], [43, 53, 1.0], [68, 77, 1.0], [143, 150, 1.0], [151, 159, 1.0], [160, 170, 1.0], [187, 195, 1.0], [196, 207, 1.0]]}}, {"doc_id": "2016.sigirconf_conference-2016.1", "score": 15.244932383389985, "relevance": 1, "rank": 8, "weights": {"doc_id": [], "default_text": [[111, 122, 1.0], [123, 132, 1.0], [495, 506, 1.0], [507, 516, 1.0], [526, 533, 1.0], [534, 542, 1.0], [543, 553, 1.0], [702, 713, 1.0], [781, 788, 1.0], [789, 797, 1.0], [1016, 1024, 1.0], [1221, 1229, 1.0], [1753, 1761, 1.0], [1956, 1963, 1.0], [1964, 1972, 1.0], [2169, 2177, 1.0], [2178, 2188, 1.0], [2237, 2244, 1.0], [2245, 2253, 1.0], [2675, 2683, 1.0], [3033, 3040, 1.0], [3041, 3049, 1.0], [3050, 3060, 1.0], [3065, 3076, 1.0], [3077, 3086, 1.0], [3165, 3173, 1.0]]}, "snippet": {"field": "default_text", "start": 490, "stop": 690, "weights": [[5, 16, 1.0], [17, 26, 1.0], [36, 43, 1.0], [44, 52, 1.0], [53, 63, 1.0]]}}, {"doc_id": "2013.ipm_journal-ir0anthology0volumeA49A4.5", "score": 15.075904911009966, "relevance": 1, "rank": 9, "weights": {"doc_id": [], "default_text": [[394, 401, 1.0], [402, 410, 1.0], [713, 720, 1.0], [721, 729, 1.0], [975, 982, 1.0], [983, 991, 1.0], [1046, 1054, 1.0], [1285, 1293, 1.0], [1797, 1804, 1.0], [1805, 1813, 1.0]]}, "snippet": {"field": "default_text", "start": 970, "stop": 1170, "weights": [[5, 12, 1.0], [13, 21, 1.0], [76, 84, 1.0]]}}, {"doc_id": "2018.sigirconf_conference-2018.88", "score": 15.055814925764452, "relevance": 1, "rank": 10, "weights": {"doc_id": [], "default_text": [[94, 105, 1.0], [106, 115, 1.0], [575, 582, 1.0], [583, 591, 1.0], [743, 750, 1.0], [751, 759, 1.0], [958, 965, 1.0], [966, 974, 1.0], [1014, 1021, 1.0], [1022, 1030, 1.0], [1213, 1220, 1.0], [1221, 1229, 1.0], [1604, 1611, 1.0], [1612, 1620, 1.0], [1770, 1777, 1.0], [1778, 1786, 1.0], [1923, 1930, 1.0], [1931, 1939, 1.0]]}, "snippet": {"field": "default_text", "start": 570, "stop": 770, "weights": [[5, 12, 1.0], [13, 21, 1.0], [173, 180, 1.0], [181, 189, 1.0]]}}], "run_2": [], "summary": [], "mergedWeights": {}}, {"fields": {"query_id": "4", "title": "recommendation systems", "description": "What documents contain information on recommendation systems (systems that suggest items to users based on their past behaviour and interests)", "narrative": "Relevant documents contain information or research on the topic of recommendation systems in the context of information retrieval.", "contrast": {"name": "singlerun", "value": 0}}, "metrics": {"P@1": [1.0], "P@3": [1.0], "P@5": [1.0], "P@10": [1.0], "nDCG@1": [1.0], "nDCG@3": [1.0], "nDCG@5": [1.0], "nDCG@10": [1.0]}, "run_1": [{"doc_id": "2020.sigirconf_conference-2020.355", "score": 11.544499746740051, "relevance": 1, "rank": 1, "weights": {"doc_id": [], "default_text": [[492, 499, 1.0], [589, 596, 1.0], [713, 720, 1.0], [810, 817, 1.0], [861, 868, 1.0], [1210, 1217, 1.0], [1351, 1365, 1.0], [1411, 1425, 1.0], [1463, 1477, 1.0], [1725, 1732, 1.0], [1799, 1806, 1.0], [2074, 2081, 1.0], [2110, 2117, 1.0], [2165, 2172, 1.0], [2232, 2239, 1.0], [2266, 2280, 1.0]]}, "snippet": {"field": "default_text", "start": 2069, "stop": 2269, "weights": [[5, 12, 1.0], [41, 48, 1.0], [96, 103, 1.0], [163, 170, 1.0], [197, 211, 1.0]]}}, {"doc_id": "2008.wwwconf_conference-2008.21", "score": 11.371858429428302, "relevance": 1, "rank": 2, "weights": {"doc_id": [], "default_text": [[591, 598, 1.0], [612, 619, 1.0], [790, 804, 1.0], [805, 812, 1.0], [883, 897, 1.0], [1060, 1067, 1.0], [1191, 1205, 1.0], [1379, 1393, 1.0], [1516, 1530, 1.0], [1579, 1593, 1.0], [1594, 1601, 1.0], [1613, 1620, 1.0], [1771, 1778, 1.0], [1816, 1823, 1.0], [2061, 2068, 1.0], [2118, 2132, 1.0], [2133, 2140, 1.0]]}, "snippet": {"field": "default_text", "start": 1511, "stop": 1711, "weights": [[5, 19, 1.0], [68, 82, 1.0], [83, 90, 1.0], [102, 109, 1.0]]}}, {"doc_id": "2002.cikm_conference-2002.7", "score": 11.309011780954993, "relevance": 1, "rank": 3, "weights": {"doc_id": [], "default_text": [[513, 520, 1.0], [703, 710, 1.0], [854, 861, 1.0], [1024, 1038, 1.0], [1122, 1136, 1.0], [1354, 1361, 1.0], [1482, 1496, 1.0], [1497, 1504, 1.0]]}, "snippet": {"field": "default_text", "start": 1349, "stop": 1549, "weights": [[5, 12, 1.0], [133, 147, 1.0], [148, 155, 1.0]]}}, {"doc_id": "2007.wwwjournals_journal-ir0anthology0volumeA10A4.3", "score": 11.196293825493319, "relevance": 1, "rank": 4, "weights": {"doc_id": [], "default_text": [[455, 462, 1.0], [640, 647, 1.0], [683, 690, 1.0], [740, 747, 1.0], [826, 840, 1.0], [931, 938, 1.0], [1128, 1135, 1.0], [1345, 1352, 1.0], [1446, 1453, 1.0], [1550, 1557, 1.0]]}, "snippet": {"field": "default_text", "start": 635, "stop": 835, "weights": [[5, 12, 1.0], [48, 55, 1.0], [105, 112, 1.0], [191, 205, 1.0]]}}, {"doc_id": "2020.wwwconf_conference-2020c.22", "score": 11.163543912141822, "relevance": 1, "rank": 5, "weights": {"doc_id": [], "default_text": [[391, 405, 1.0], [537, 544, 1.0], [817, 831, 1.0], [910, 924, 1.0], [925, 932, 1.0], [995, 1009, 1.0], [1010, 1017, 1.0], [1162, 1169, 1.0], [1184, 1191, 1.0], [1253, 1267, 1.0], [1268, 1275, 1.0]]}, "snippet": {"field": "default_text", "start": 812, "stop": 1012, "weights": [[5, 19, 1.0], [98, 112, 1.0], [113, 120, 1.0], [183, 197, 1.0], [198, 205, 1.0]]}}, {"doc_id": "2021.wsdm_conference-2021.141", "score": 11.046996942971946, "relevance": 1, "rank": 6, "weights": {"doc_id": [], "default_text": [[462, 469, 1.0], [568, 582, 1.0], [1215, 1229, 1.0], [1410, 1417, 1.0], [1432, 1439, 1.0], [1456, 1470, 1.0]]}, "snippet": {"field": "default_text", "start": 1405, "stop": 1605, "weights": [[5, 12, 1.0], [27, 34, 1.0], [51, 65, 1.0]]}}, {"doc_id": "2018.wwwjournals_journal-ir0anthology0volumeA21A4.5", "score": 11.036375654231955, "relevance": 1, "rank": 7, "weights": {"doc_id": [], "default_text": [[661, 668, 1.0], [790, 797, 1.0], [1447, 1454, 1.0], [1559, 1573, 1.0], [1620, 1634, 1.0]]}, "snippet": {"field": "default_text", "start": 1442, "stop": 1642, "weights": [[5, 12, 1.0], [117, 131, 1.0], [178, 192, 1.0]]}}, {"doc_id": "2011.wsdm_conference-2011.36", "score": 10.995970595232922, "relevance": 1, "rank": 8, "weights": {"doc_id": [], "default_text": [[473, 480, 1.0], [574, 581, 1.0], [674, 681, 1.0], [918, 925, 1.0], [993, 1000, 1.0], [1029, 1036, 1.0], [1132, 1139, 1.0], [1534, 1541, 1.0]]}, "snippet": {"field": "default_text", "start": 468, "stop": 668, "weights": [[5, 12, 1.0], [106, 113, 1.0]]}}, {"doc_id": "2015.clef_conference-2015w.55", "score": 10.992084996304062, "relevance": 1, "rank": 9, "weights": {"doc_id": [], "default_text": [[522, 529, 1.0], [639, 646, 1.0], [961, 975, 1.0], [1069, 1076, 1.0], [1440, 1454, 1.0]]}, "snippet": {"field": "default_text", "start": 517, "stop": 717, "weights": [[5, 12, 1.0], [122, 129, 1.0]]}}, {"doc_id": "2015.cikm_conference-2015.150", "score": 10.975707450789628, "relevance": 1, "rank": 10, "weights": {"doc_id": [], "default_text": [[503, 510, 1.0], [967, 981, 1.0], [1182, 1189, 1.0], [1273, 1287, 1.0], [1993, 2000, 1.0]]}, "snippet": {"field": "default_text", "start": 1177, "stop": 1377, "weights": [[5, 12, 1.0], [96, 110, 1.0]]}}], "run_2": [], "summary": [], "mergedWeights": {}}], "docs": {"2005.clef_workshop-2005w.8": {"doc_id": "2005.clef_workshop-2005w.8", "default_text": "DBLP:conf/clef/2005w Working Notes for CLEF 2005 Workshop co-located with the 9th European Conference on Digital Libraries (ECDL 2005), Wien, Austria, September 21-22, 2005 CEUR Workshop Proceedings 1171 CEUR-WS.org 2005 http://ceur-ws.org/Vol-1171/CLEF2005wn-adhoc-LiomaEt2005.pdf https://dblp.org/rec/conf/clef/LiomaMHPO05a.bib dblp computer science bibliography, https://dblp.org DBLP:conf/clef/LiomaMHPO05a inproceedings http://ceur-ws.org/Vol-1171/CLEF2005wn-adhoc-LiomaEt2005.pdf Christina Lioma Craig Macdonald Ben He Vassilis Plachouras Iadh Ounis CLEF 1581522270.0 In the CLEF 2005 Ad-Hoc Track we experimented with language-specific morphosyntactic processing and light Natural Language Processing (NLP) for the retrieval of Bulgarian, French, Italian, English and Greek. Applying Light Natural Language Processing to Ad-Hoc Cross Language Information Retrieval ", "text": "DBLP:conf/clef/2005w Working Notes for CLEF 2005 Workshop co-located with the 9th European Conference on Digital Libraries (ECDL 2005), Wien, Austria, September 21-22, 2005 CEUR Workshop Proceedings 1171 CEUR-WS.org 2005 http://ceur-ws.org/Vol-1171/CLEF2005wn-adhoc-LiomaEt2005.pdf https://dblp.org/rec/conf/clef/LiomaMHPO05a.bib dblp computer science bibliography, https://dblp.org DBLP:conf/clef/LiomaMHPO05a inproceedings http://ceur-ws.org/Vol-1171/CLEF2005wn-adhoc-LiomaEt2005.pdf Christina Lioma Craig Macdonald Ben He Vassilis Plachouras Iadh Ounis CLEF 1581522270.0 In the CLEF 2005 Ad-Hoc Track we experimented with language-specific morphosyntactic processing and light Natural Language Processing (NLP) for the retrieval of Bulgarian, French, Italian, English and Greek. Applying Light Natural Language Processing to Ad-Hoc Cross Language Information Retrieval "}, "1997.cikm_workshop-97.16": {"doc_id": "1997.cikm_workshop-97.16", "default_text": "DBLP:conf/npiv/97 Proceedings of the Sixth International Conference on Information and Knowledge Management (CIKM'97), Las Vegas, Nevada, USA, November 10-14, 1997 49\u201351 ACM 1997 https://doi.org/10.1145/275519.275530 10.1145/275519.275530 https://dblp.org/rec/conf/cikm/Kimoto97.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/Kimoto97 inproceedings Haruo Kimoto CIKM 1541519872.0 AbstractThis paper describes a new method of multimedia information retrieval which shows multimedia information on a screen in a multi-directional way, using perspective, during the process of recursive interactive retrieval. This method supports a graphical, sensitive and individually tailored interaction between a user and multimedia data. The original work here in is the support of interaction between a user and multimedia data in a way that shows as many as multimedia data in a multi-directional manner on a screen. This work does not involve analytical techniques, although much current research into multimedia information retrieval focuses on processing and handling multimedia data. The prototype system INMUL(Interactive Multidirectional Information Displaying System) is now being implemented. 3D Visualization Technique for Retrieval of Documents and Images ", "text": "DBLP:conf/npiv/97 Proceedings of the Sixth International Conference on Information and Knowledge Management (CIKM'97), Las Vegas, Nevada, USA, November 10-14, 1997 49\u201351 ACM 1997 https://doi.org/10.1145/275519.275530 10.1145/275519.275530 https://dblp.org/rec/conf/cikm/Kimoto97.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/Kimoto97 inproceedings Haruo Kimoto CIKM 1541519872.0 AbstractThis paper describes a new method of multimedia information retrieval which shows multimedia information on a screen in a multi-directional way, using perspective, during the process of recursive interactive retrieval. This method supports a graphical, sensitive and individually tailored interaction between a user and multimedia data. The original work here in is the support of interaction between a user and multimedia data in a way that shows as many as multimedia data in a multi-directional manner on a screen. This work does not involve analytical techniques, although much current research into multimedia information retrieval focuses on processing and handling multimedia data. The prototype system INMUL(Interactive Multidirectional Information Displaying System) is now being implemented. 3D Visualization Technique for Retrieval of Documents and Images "}, "2009.wwwconf_workshop-2009ldow.10": {"doc_id": "2009.wwwconf_workshop-2009ldow.10", "default_text": "DBLP:conf/www/2009ldow Proceedings of the WWW2009 Workshop on Linked Data on the Web, LDOW 2009, Madrid, Spain, April 20, 2009 CEUR Workshop Proceedings 538 CEUR-WS.org 2009 http://ceur-ws.org/Vol-538/ldow2009_paper17.pdf https://dblp.org/rec/conf/www/HausenblasTBR09.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/HausenblasTBR09 inproceedings http://ceur-ws.org/Vol-538/ldow2009_paper17.pdf Michael Hausenblas Rapha\u00ebl Troncy Tobias B\u00fcrger Yves Raimond WWW 1581522262.0 In this paper, we introduce interlinking multimedia (iM), a pragmatic way to apply the linked data principles to fragments of multimedia items. We report on use cases showing the need for retrieving and describing multimedia fragments. We then introduce the principles for interlinking multimedia in the Web of Data, discussing potential solutions which sometimes highlight controversial debates regarding what the various representations of a Web resource span. We finally present methods for enabling a widespread use of interlinking multimedia. Interlinking Multimedia: How to Apply Linked Data Principles to Multimedia Fragments ", "text": "DBLP:conf/www/2009ldow Proceedings of the WWW2009 Workshop on Linked Data on the Web, LDOW 2009, Madrid, Spain, April 20, 2009 CEUR Workshop Proceedings 538 CEUR-WS.org 2009 http://ceur-ws.org/Vol-538/ldow2009_paper17.pdf https://dblp.org/rec/conf/www/HausenblasTBR09.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/HausenblasTBR09 inproceedings http://ceur-ws.org/Vol-538/ldow2009_paper17.pdf Michael Hausenblas Rapha\u00ebl Troncy Tobias B\u00fcrger Yves Raimond WWW 1581522262.0 In this paper, we introduce interlinking multimedia (iM), a pragmatic way to apply the linked data principles to fragments of multimedia items. We report on use cases showing the need for retrieving and describing multimedia fragments. We then introduce the principles for interlinking multimedia in the Web of Data, discussing potential solutions which sometimes highlight controversial debates regarding what the various representations of a Web resource span. We finally present methods for enabling a widespread use of interlinking multimedia. Interlinking Multimedia: How to Apply Linked Data Principles to Multimedia Fragments "}, "2012.sigirconf_conference-2012.50": {"doc_id": "2012.sigirconf_conference-2012.50", "default_text": "DBLP:conf/sigir/2012 The 35th International ACM SIGIR conference on research and development in Information Retrieval, SIGIR '12, Portland, OR, USA, August 12-16, 2012 475\u2013484 ACM 2012 https://doi.org/10.1145/2348283.2348349 10.1145/2348283.2348349 https://dblp.org/rec/conf/sigir/QumsiyehN12.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/QumsiyehN12 inproceedings Rani Qumsiyeh Yiu-Kai Ng SIGIR 1542189490.0 ABSTRACTExisting multimedia recommenders suggest a specific type of multimedia items rather than items of different types personalized for a user based on his/her preference. Assume that a user is interested in a particular family movie, it is appealing if a multimedia recommendation system can suggest other movies, music, books, and paintings closely related to the movie. We propose a comprehensive, personalized multimedia recommendation system, denoted MudRecS, which makes recommendations on movies, music, books, and paintings similar in content to other movies, music, books, and/or paintings that a MudRecS user is interested in. MudRecS does not rely on users' access patterns/histories, connection information extracted from social networking sites, collaborated filtering methods, or user personal attributes (such as gender and age) to perform the recommendation task. It simply considers the users' ratings, genres, role players (authors or artists), and reviews of different multimedia items, which are abundant and easy to find on the Web. MudRecS predicts the ratings of multimedia items that match the interests of a user to make recommendations. The performance of MudRecS has been compared with current state-ofthe-art multimedia recommenders using various multimedia datasets, and the experimental results show that MudRecS significantly outperforms other systems in accurately predicting the ratings of multimedia items to be recommended. Predicting the ratings of multimedia items for making personalized recommendations ", "text": "DBLP:conf/sigir/2012 The 35th International ACM SIGIR conference on research and development in Information Retrieval, SIGIR '12, Portland, OR, USA, August 12-16, 2012 475\u2013484 ACM 2012 https://doi.org/10.1145/2348283.2348349 10.1145/2348283.2348349 https://dblp.org/rec/conf/sigir/QumsiyehN12.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/QumsiyehN12 inproceedings Rani Qumsiyeh Yiu-Kai Ng SIGIR 1542189490.0 ABSTRACTExisting multimedia recommenders suggest a specific type of multimedia items rather than items of different types personalized for a user based on his/her preference. Assume that a user is interested in a particular family movie, it is appealing if a multimedia recommendation system can suggest other movies, music, books, and paintings closely related to the movie. We propose a comprehensive, personalized multimedia recommendation system, denoted MudRecS, which makes recommendations on movies, music, books, and paintings similar in content to other movies, music, books, and/or paintings that a MudRecS user is interested in. MudRecS does not rely on users' access patterns/histories, connection information extracted from social networking sites, collaborated filtering methods, or user personal attributes (such as gender and age) to perform the recommendation task. It simply considers the users' ratings, genres, role players (authors or artists), and reviews of different multimedia items, which are abundant and easy to find on the Web. MudRecS predicts the ratings of multimedia items that match the interests of a user to make recommendations. The performance of MudRecS has been compared with current state-ofthe-art multimedia recommenders using various multimedia datasets, and the experimental results show that MudRecS significantly outperforms other systems in accurately predicting the ratings of multimedia items to be recommended. Predicting the ratings of multimedia items for making personalized recommendations "}, "2018.sigirconf_conference-2018.30": {"doc_id": "2018.sigirconf_conference-2018.30", "default_text": "DBLP:conf/sigir/2018 The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, SIGIR 2018, Ann Arbor, MI, USA, July 08-12, 2018 275\u2013284 ACM 2018 https://doi.org/10.1145/3209978.3210037 10.1145/3209978.3210037 https://dblp.org/rec/conf/sigir/VoL18.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/VoL18 inproceedings Nguyen Vo Kyumin Lee SIGIR 1600256062.0 ABSTRACTA large body of research work and efforts have been focused on detecting fake news and building online fact-check systems in order to debunk fake news as soon as possible. Despite the existence of these systems, fake news is still wildly shared by online users. It indicates that these systems may not be fully utilized. After detecting fake news, what is the next step to stop people from sharing it? How can we improve the utilization of these fact-check systems? To fill this gap, in this paper, we (i) collect and analyze online users called guardians, who correct misinformation and fake news in online discussions by referring fact-checking URLs; and (ii) propose a novel fact-checking URL recommendation model to encourage the guardians to engage more in fact-checking activities. We found that the guardians usually took less than one day to reply to claims in online conversations and took another day to spread verified information to hundreds of millions of followers. Our proposed recommendation model outperformed four state-of-the-art models by 11%\u223c33%. Our source code and dataset are available at The Rise of Guardians: Fact-checking URL Recommendation to Combat Fake News ", "text": "DBLP:conf/sigir/2018 The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, SIGIR 2018, Ann Arbor, MI, USA, July 08-12, 2018 275\u2013284 ACM 2018 https://doi.org/10.1145/3209978.3210037 10.1145/3209978.3210037 https://dblp.org/rec/conf/sigir/VoL18.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/VoL18 inproceedings Nguyen Vo Kyumin Lee SIGIR 1600256062.0 ABSTRACTA large body of research work and efforts have been focused on detecting fake news and building online fact-check systems in order to debunk fake news as soon as possible. Despite the existence of these systems, fake news is still wildly shared by online users. It indicates that these systems may not be fully utilized. After detecting fake news, what is the next step to stop people from sharing it? How can we improve the utilization of these fact-check systems? To fill this gap, in this paper, we (i) collect and analyze online users called guardians, who correct misinformation and fake news in online discussions by referring fact-checking URLs; and (ii) propose a novel fact-checking URL recommendation model to encourage the guardians to engage more in fact-checking activities. We found that the guardians usually took less than one day to reply to claims in online conversations and took another day to spread verified information to hundreds of millions of followers. Our proposed recommendation model outperformed four state-of-the-art models by 11%\u223c33%. Our source code and dataset are available at The Rise of Guardians: Fact-checking URL Recommendation to Combat Fake News "}, "2018.sigirconf_conference-2018.88": {"doc_id": "2018.sigirconf_conference-2018.88", "default_text": "DBLP:conf/sigir/2018 The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, SIGIR 2018, Ann Arbor, MI, USA, July 08-12, 2018 855\u2013864 ACM 2018 https://doi.org/10.1145/3209978.3210013 10.1145/3209978.3210013 https://dblp.org/rec/conf/sigir/SuAWW18.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/SuAWW18 inproceedings Yu Su Ahmed Hassan Awadallah Miaosen Wang Ryen W. White SIGIR 1600256062.0 ABSTRACTThe rapidly increasing ubiquity of computing puts a great demand on next-generation human-machine interfaces. Natural language interfaces, exemplified by virtual assistants like Apple Siri and Microsoft Cortana, are widely believed to be a promising direction. However, current natural language interfaces provide users with little help in case of incorrect interpretation of user commands. We hypothesize that the support of fine-grained user interaction can greatly improve the usability of natural language interfaces. In the specific setting of natural language interfaces to web APIs, we conduct a systematic study to verify our hypothesis. To facilitate this study, we propose a novel modular sequence-to-sequence model to create interactive natural language interfaces. By decomposing the complex prediction process of a typical sequence-to-sequence model into small, highly-specialized prediction units called modules, it becomes straightforward to explain the model prediction to the user, and solicit user feedback to correct possible prediction errors at a fine-grained level. We test our hypothesis by comparing an interactive natural language interface with its non-interactive version through both simulation and human subject experiments with real-world APIs. We show that with interactive natural language interfaces, users can achieve a higher success rate and a lower task completion time, which lead to greatly improved user satisfaction. Natural Language Interfaces with Fine-Grained User Interaction: A Case Study on Web APIs ", "text": "DBLP:conf/sigir/2018 The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, SIGIR 2018, Ann Arbor, MI, USA, July 08-12, 2018 855\u2013864 ACM 2018 https://doi.org/10.1145/3209978.3210013 10.1145/3209978.3210013 https://dblp.org/rec/conf/sigir/SuAWW18.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/SuAWW18 inproceedings Yu Su Ahmed Hassan Awadallah Miaosen Wang Ryen W. White SIGIR 1600256062.0 ABSTRACTThe rapidly increasing ubiquity of computing puts a great demand on next-generation human-machine interfaces. Natural language interfaces, exemplified by virtual assistants like Apple Siri and Microsoft Cortana, are widely believed to be a promising direction. However, current natural language interfaces provide users with little help in case of incorrect interpretation of user commands. We hypothesize that the support of fine-grained user interaction can greatly improve the usability of natural language interfaces. In the specific setting of natural language interfaces to web APIs, we conduct a systematic study to verify our hypothesis. To facilitate this study, we propose a novel modular sequence-to-sequence model to create interactive natural language interfaces. By decomposing the complex prediction process of a typical sequence-to-sequence model into small, highly-specialized prediction units called modules, it becomes straightforward to explain the model prediction to the user, and solicit user feedback to correct possible prediction errors at a fine-grained level. We test our hypothesis by comparing an interactive natural language interface with its non-interactive version through both simulation and human subject experiments with real-world APIs. We show that with interactive natural language interfaces, users can achieve a higher success rate and a lower task completion time, which lead to greatly improved user satisfaction. Natural Language Interfaces with Fine-Grained User Interaction: A Case Study on Web APIs "}, "2013.sigirconf_conference-2013.1": {"doc_id": "2013.sigirconf_conference-2013.1", "default_text": "DBLP:conf/sigir/2013 The 36th International ACM SIGIR conference on research and development in Information Retrieval, SIGIR '13, Dublin, Ireland - July 28 - August 01, 2013 1\u20132 ACM 2013 https://doi.org/10.1145/2484028.2494492 10.1145/2484028.2494492 https://dblp.org/rec/conf/sigir/Smith13.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/Smith13 inproceedings John R. Smith SIGIR 1541498844.0 ABSTRACTAcross multiple generations of information technology that have dealt with structured and unstructured data, the explosion of multimedia data is creating the biggest wave of all. Huge volumes of multimedia -images, video and audio are being generated and consumed daily. Currently, multimedia makes up 60% of internet traffic, 70% of mobile phone traffic and 70% of all available unstructured data. To give specific examples, Web users are uploading 72 video-hours to YouTube per minute, and on an average day, social media users post 300 hundred million photos to Facebook. Consumers using mobile phones and digital cameras are taking 500 billion photos per year, or 78 per person on the planet . Specialized domains are participating too. Medical institutions are acquiring one billion radiological images per year, and cities are installing hundreds of millions of video cameras worldwide for safety, security and law enforcement. Industries across life sciences, petroleum exploration, astronomy, insurance, retail and many others are faced with huge and growing volumes of multimedia data.Multimedia clearly is \"big data\". But, it it big data not just because there is a lot of it. Multimedia is big data because increasingly it is becoming a valuable source for insights and information. Multimedia data can tell us about things happening in the world, point out places, events or topics of interest (memes), give clues about a person's preferences and even capture a rolling log of human history . However, the challenge with multimedia big data is that images, video and audio require much more sophisticated algorithms for content analysis than previous waves of structured and unstructured data. This is spurring on a tremendous amount of research on efficient and effective techniques for \"bridging the semantic gap\" to enable large-scale multimedia information extraction and retrieval .In this talk we present a perspective across multiple industry problems, including safety and security, medical, Web, social and mobile media, and motivate the need for largescale analysis and retrieval of multimedia data. We dePermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage, and that copies bear this notice and the full citation on the first page. Copyrights for thirdparty components of this work must be honored. For all other uses, contact the owner/author(s). Copyright is held by the author/owner(s). SIGIR'13, July 28-August 1, 2013, Dublin, Ireland. ACM 978-1-4503-2034-4/13/07. scribe a multi-layer architecture that incorporates capabilities for audio-visual feature extraction, machine learning and semantic modeling and provides a powerful framework for learning and classifying contents of multimedia data. We discuss the role semantic ontologies for representing audiovisual concepts and relationships, which are essential for training semantic classifiers . We discuss the importance of using faceted classification schemes in particular for organizing multimedia semantic concepts in order to achieve effective learning and retrieval . We also show how training and scoring of multimedia semantics can be implemented on big data distributed computing platforms to address both massive-scale analysis and low-latency processing . We describe multiple efforts at IBM on image and video analysis and retrieval, including IBM Multimedia Analysis and Retrieval System (IMARS), and show recent results for semanticbased classification and retrieval. We conclude with future directions for improving analysis of multimedia through interactive and curriculum-based techniques for multimedia semantics-based learning and retrieval. Riding the multimedia big data wave ", "text": "DBLP:conf/sigir/2013 The 36th International ACM SIGIR conference on research and development in Information Retrieval, SIGIR '13, Dublin, Ireland - July 28 - August 01, 2013 1\u20132 ACM 2013 https://doi.org/10.1145/2484028.2494492 10.1145/2484028.2494492 https://dblp.org/rec/conf/sigir/Smith13.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/Smith13 inproceedings John R. Smith SIGIR 1541498844.0 ABSTRACTAcross multiple generations of information technology that have dealt with structured and unstructured data, the explosion of multimedia data is creating the biggest wave of all. Huge volumes of multimedia -images, video and audio are being generated and consumed daily. Currently, multimedia makes up 60% of internet traffic, 70% of mobile phone traffic and 70% of all available unstructured data. To give specific examples, Web users are uploading 72 video-hours to YouTube per minute, and on an average day, social media users post 300 hundred million photos to Facebook. Consumers using mobile phones and digital cameras are taking 500 billion photos per year, or 78 per person on the planet . Specialized domains are participating too. Medical institutions are acquiring one billion radiological images per year, and cities are installing hundreds of millions of video cameras worldwide for safety, security and law enforcement. Industries across life sciences, petroleum exploration, astronomy, insurance, retail and many others are faced with huge and growing volumes of multimedia data.Multimedia clearly is \"big data\". But, it it big data not just because there is a lot of it. Multimedia is big data because increasingly it is becoming a valuable source for insights and information. Multimedia data can tell us about things happening in the world, point out places, events or topics of interest (memes), give clues about a person's preferences and even capture a rolling log of human history . However, the challenge with multimedia big data is that images, video and audio require much more sophisticated algorithms for content analysis than previous waves of structured and unstructured data. This is spurring on a tremendous amount of research on efficient and effective techniques for \"bridging the semantic gap\" to enable large-scale multimedia information extraction and retrieval .In this talk we present a perspective across multiple industry problems, including safety and security, medical, Web, social and mobile media, and motivate the need for largescale analysis and retrieval of multimedia data. We dePermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage, and that copies bear this notice and the full citation on the first page. Copyrights for thirdparty components of this work must be honored. For all other uses, contact the owner/author(s). Copyright is held by the author/owner(s). SIGIR'13, July 28-August 1, 2013, Dublin, Ireland. ACM 978-1-4503-2034-4/13/07. scribe a multi-layer architecture that incorporates capabilities for audio-visual feature extraction, machine learning and semantic modeling and provides a powerful framework for learning and classifying contents of multimedia data. We discuss the role semantic ontologies for representing audiovisual concepts and relationships, which are essential for training semantic classifiers . We discuss the importance of using faceted classification schemes in particular for organizing multimedia semantic concepts in order to achieve effective learning and retrieval . We also show how training and scoring of multimedia semantics can be implemented on big data distributed computing platforms to address both massive-scale analysis and low-latency processing . We describe multiple efforts at IBM on image and video analysis and retrieval, including IBM Multimedia Analysis and Retrieval System (IMARS), and show recent results for semanticbased classification and retrieval. We conclude with future directions for improving analysis of multimedia through interactive and curriculum-based techniques for multimedia semantics-based learning and retrieval. Riding the multimedia big data wave "}, "2016.sigirconf_conference-2016.1": {"doc_id": "2016.sigirconf_conference-2016.1", "default_text": "DBLP:conf/sigir/2016 Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval, SIGIR 2016, Pisa, Italy, July 17-21, 2016 1 ACM 2016 https://doi.org/10.1145/2911451.2926732 10.1145/2911451.2926732 https://dblp.org/rec/conf/sigir/Manning16.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/Manning16 inproceedings Christopher D. Manning SIGIR 1542189491.0 ABSTRACTThere is a lot of overlap between the core problems of information retrieval (IR) and natural language processing (NLP). An IR system gains from understanding a user need and from understanding documents, and hence being able to determine whether a document has information that satisfies the user need. Much of NLP is about the same thing: Natural language understanding aims to understand the meaning of questions and documents and meaning relationships. The exciting recent application of deep learning approaches in NLP has brought new tools for effectively understanding language semantics. In principle, there should be a lot of synergy, though in practice the concerns of IR on large systems and macro-scale understanding have tended to contrast with the emphasis in NLP on language structure and micro-scale understanding.My talk will emphasize the two topics of how NLP can contribute to understanding textual relationships and how deep learning approaches substantially aid in this goal. One basic -and very successful tool -has been the new generation of distributed word representations: neural word embeddings. However, beyond just word meanings, we need to understand how to compose the meanings of larger pieces of text. Two requirements for that are good ways to understand the structure of human language utterances and ways to compose their meanings. Deep learning methods can help for both tasks. Finally, we need to understand relationships between pieces of text, to be able to do tasks such as Natural Language Inference (or Recognizing Textual Entailment) and Question Answering, and I will look at some of our recent work in these areas, both with and without the help of neural networks.\nKeywordsNatural language processing, deep learning, word vectors, compositionality, natural language inference, recognizing textual entailment, question answering\nShort BiographyChristopher Manning is a professor of computer science and linguistics at Stanford University. His Ph.D. is from Stanford in 1995, and he held faculty positions at Carnegie Mellon University and the University of Sydney before returning to Stanford. His research goal is computers that can intelligently process, understand, and generate human language material. Manning concentrates on machine learning approaches to computational linguistic problems, including syntactic parsing, computational semantics and pragmatics, textual inference, machine translation, and using deep learning for NLP. He is an ACM Fellow, a AAAI Fellow, and an ACL Fellow, and has coauthored leading textbooks on statistical natural language processing and information retrieval. He is a member of the Stanford NLP group (@stanfordnlp). Understanding Human Language: Can NLP and Deep Learning Help? ", "text": "DBLP:conf/sigir/2016 Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval, SIGIR 2016, Pisa, Italy, July 17-21, 2016 1 ACM 2016 https://doi.org/10.1145/2911451.2926732 10.1145/2911451.2926732 https://dblp.org/rec/conf/sigir/Manning16.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/Manning16 inproceedings Christopher D. Manning SIGIR 1542189491.0 ABSTRACTThere is a lot of overlap between the core problems of information retrieval (IR) and natural language processing (NLP). An IR system gains from understanding a user need and from understanding documents, and hence being able to determine whether a document has information that satisfies the user need. Much of NLP is about the same thing: Natural language understanding aims to understand the meaning of questions and documents and meaning relationships. The exciting recent application of deep learning approaches in NLP has brought new tools for effectively understanding language semantics. In principle, there should be a lot of synergy, though in practice the concerns of IR on large systems and macro-scale understanding have tended to contrast with the emphasis in NLP on language structure and micro-scale understanding.My talk will emphasize the two topics of how NLP can contribute to understanding textual relationships and how deep learning approaches substantially aid in this goal. One basic -and very successful tool -has been the new generation of distributed word representations: neural word embeddings. However, beyond just word meanings, we need to understand how to compose the meanings of larger pieces of text. Two requirements for that are good ways to understand the structure of human language utterances and ways to compose their meanings. Deep learning methods can help for both tasks. Finally, we need to understand relationships between pieces of text, to be able to do tasks such as Natural Language Inference (or Recognizing Textual Entailment) and Question Answering, and I will look at some of our recent work in these areas, both with and without the help of neural networks.\nKeywordsNatural language processing, deep learning, word vectors, compositionality, natural language inference, recognizing textual entailment, question answering\nShort BiographyChristopher Manning is a professor of computer science and linguistics at Stanford University. His Ph.D. is from Stanford in 1995, and he held faculty positions at Carnegie Mellon University and the University of Sydney before returning to Stanford. His research goal is computers that can intelligently process, understand, and generate human language material. Manning concentrates on machine learning approaches to computational linguistic problems, including syntactic parsing, computational semantics and pragmatics, textual inference, machine translation, and using deep learning for NLP. He is an ACM Fellow, a AAAI Fellow, and an ACL Fellow, and has coauthored leading textbooks on statistical natural language processing and information retrieval. He is a member of the Stanford NLP group (@stanfordnlp). Understanding Human Language: Can NLP and Deep Learning Help? "}, "2020.sigirconf_conference-2020.355": {"doc_id": "2020.sigirconf_conference-2020.355", "default_text": "DBLP:conf/sigir/2020 Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 2425\u20132428 ACM 2020 https://doi.org/10.1145/3397271.3401419 10.1145/3397271.3401419 https://dblp.org/rec/conf/sigir/Lei0RC20.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/Lei0RC20 inproceedings Wenqiang Lei Xiangnan He Maarten de Rijke Tat-Seng Chua SIGIR 1619422021.0 Recommender systems have demonstrated great success in information seeking. However, traditional recommender systems work in a static way, estimating user preferences on items from past interaction history. This prevents recommender systems from capturing dynamic and fine-grained preferences of users. Conversational recommender systems bring a revolution to existing recommender systems. They are able to communicate with users through natural languages during which they can explicitly ask whether a user likes an attribute or not. With the preferred attributes, a recommender system can conduct more accurate and personalized recommendations. Therefore, while they are still a relatively new topic, conversational recommender systems attract great research attention. We identify four emerging directions: (1) exploration and exploitation trade-off in the cold-start recommendation setting; (2) attributecentric conversational recommendation; (3) strategy-focused conversational recommendation; and (4) dialogue understanding and response generation. This tutorial covers these four directions, providing a review of existing approaches and progress on the topic. By presenting the emerging and promising topic of conversational recommender systems, we aim to provide take-aways to practitioners to build their own systems. We also want to stimulate more ideas and discussions with audiences on core problems of this topic such as task formalization, dataset collection, algorithm development, and evaluation, with the ambition of facilitating the development of conversational recommender systems. CCS CONCEPTS \u2022 Information systems \u2192 Users and interactive retrieval; Recommender systems; Personalization; \u2022 Human-centered computing \u2192 Interactive systems and tools. Conversational Recommendation: Formulation, Methods, and Evaluation ", "text": "DBLP:conf/sigir/2020 Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 2425\u20132428 ACM 2020 https://doi.org/10.1145/3397271.3401419 10.1145/3397271.3401419 https://dblp.org/rec/conf/sigir/Lei0RC20.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/Lei0RC20 inproceedings Wenqiang Lei Xiangnan He Maarten de Rijke Tat-Seng Chua SIGIR 1619422021.0 Recommender systems have demonstrated great success in information seeking. However, traditional recommender systems work in a static way, estimating user preferences on items from past interaction history. This prevents recommender systems from capturing dynamic and fine-grained preferences of users. Conversational recommender systems bring a revolution to existing recommender systems. They are able to communicate with users through natural languages during which they can explicitly ask whether a user likes an attribute or not. With the preferred attributes, a recommender system can conduct more accurate and personalized recommendations. Therefore, while they are still a relatively new topic, conversational recommender systems attract great research attention. We identify four emerging directions: (1) exploration and exploitation trade-off in the cold-start recommendation setting; (2) attributecentric conversational recommendation; (3) strategy-focused conversational recommendation; and (4) dialogue understanding and response generation. This tutorial covers these four directions, providing a review of existing approaches and progress on the topic. By presenting the emerging and promising topic of conversational recommender systems, we aim to provide take-aways to practitioners to build their own systems. We also want to stimulate more ideas and discussions with audiences on core problems of this topic such as task formalization, dataset collection, algorithm development, and evaluation, with the ambition of facilitating the development of conversational recommender systems. CCS CONCEPTS \u2022 Information systems \u2192 Users and interactive retrieval; Recommender systems; Personalization; \u2022 Human-centered computing \u2192 Interactive systems and tools. Conversational Recommendation: Formulation, Methods, and Evaluation "}, "2015.clef_conference-2015w.55": {"doc_id": "2015.clef_conference-2015w.55", "default_text": "DBLP:conf/clef/2015w Working Notes of CLEF 2015 - Conference and Labs of the Evaluation forum, Toulouse, France, September 8-11, 2015 CEUR Workshop Proceedings 1391 CEUR-WS.org 2015 http://ceur-ws.org/Vol-1391/149-CR.pdf https://dblp.org/rec/conf/clef/VerbitskiyPL15.bib dblp computer science bibliography, https://dblp.org DBLP:conf/clef/VerbitskiyPL15 inproceedings http://ceur-ws.org/Vol-1391/149-CR.pdf Ilya Verbitskiy Patrick Probst Andreas Lommatzsch CLEF 1581522270.0 The development of highly scalable recommender systems, able to deliver recommendations in real time, is a challenging task. In contrast to traditional recommender systems, recommending news entails additional requirements. These requirements include tight response times, heavy load peaks, and continuously changing collections of users and items. In this paper we describe our participation at the CLEF-NewsREEL challenge 2015. We present our highly scalable implementation of a news recommendation algorithm. The developed approach alleviates all the specific challenges of news recommender systems. We use the Akka framework to build an asynchronous, distributable system able to run concurrently on multiple machines. Based on the framework a time window-based, most popular algorithm for recommending news articles is implemented. The evaluation shows that our system implemented using the Akka framework scales well with the restrictions and outperforms the recommendation precision of the baseline recommender. Development and Evaluation of a Highly Scalable News Recommender System ", "text": "DBLP:conf/clef/2015w Working Notes of CLEF 2015 - Conference and Labs of the Evaluation forum, Toulouse, France, September 8-11, 2015 CEUR Workshop Proceedings 1391 CEUR-WS.org 2015 http://ceur-ws.org/Vol-1391/149-CR.pdf https://dblp.org/rec/conf/clef/VerbitskiyPL15.bib dblp computer science bibliography, https://dblp.org DBLP:conf/clef/VerbitskiyPL15 inproceedings http://ceur-ws.org/Vol-1391/149-CR.pdf Ilya Verbitskiy Patrick Probst Andreas Lommatzsch CLEF 1581522270.0 The development of highly scalable recommender systems, able to deliver recommendations in real time, is a challenging task. In contrast to traditional recommender systems, recommending news entails additional requirements. These requirements include tight response times, heavy load peaks, and continuously changing collections of users and items. In this paper we describe our participation at the CLEF-NewsREEL challenge 2015. We present our highly scalable implementation of a news recommendation algorithm. The developed approach alleviates all the specific challenges of news recommender systems. We use the Akka framework to build an asynchronous, distributable system able to run concurrently on multiple machines. Based on the framework a time window-based, most popular algorithm for recommending news articles is implemented. The evaluation shows that our system implemented using the Akka framework scales well with the restrictions and outperforms the recommendation precision of the baseline recommender. Development and Evaluation of a Highly Scalable News Recommender System "}, "2020.clef_conference-2020w.24": {"doc_id": "2020.clef_conference-2020w.24", "default_text": "DBLP:conf/clef/2020w Working Notes of CLEF 2020 - Conference and Labs of the Evaluation Forum, Thessaloniki, Greece, September 22-25, 2020 CEUR Workshop Proceedings 2696 CEUR-WS.org 2020 http://ceur-ws.org/Vol-2696/paper_217.pdf https://dblp.org/rec/conf/clef/Moreno-Sandoval20.bib dblp computer science bibliography, https://dblp.org DBLP:conf/clef/Moreno-Sandoval20 inproceedings http://ceur-ws.org/Vol-2696/paper_217.pdf Luis Gabriel Moreno-Sandoval Edwin A. Puertas Del Puertas Alexandra Pomares Quimbaya Jorge Andr\u00e9s Alvarado-Valencia CLEF 1603815168.0 The explosive growth of fake news on social networks has aroused great interest from researchers in different disciplines. To achieve efficient and effective detection of fake news requires scientific contributions from various disciplines, such as computational linguistics, artificial intelligence, and sociology. Here we illustrate how polarity, emotion, and user statistics can be used to detect fake profiles on Twitter's social network. This paper presents a novel strategy for the characterization of the Twitter profile based on the generation of an assembly of polarity, emotion, and user statistics characteristics that serve as input to a set of classifiers. The results are part of our participation in the PAN 2020 in the CLEF in the task of Profiling Fake News Spreaders on Twitter. Assembly of Polarity, Emotion and User Statistics for Detection of Fake Profiles ", "text": "DBLP:conf/clef/2020w Working Notes of CLEF 2020 - Conference and Labs of the Evaluation Forum, Thessaloniki, Greece, September 22-25, 2020 CEUR Workshop Proceedings 2696 CEUR-WS.org 2020 http://ceur-ws.org/Vol-2696/paper_217.pdf https://dblp.org/rec/conf/clef/Moreno-Sandoval20.bib dblp computer science bibliography, https://dblp.org DBLP:conf/clef/Moreno-Sandoval20 inproceedings http://ceur-ws.org/Vol-2696/paper_217.pdf Luis Gabriel Moreno-Sandoval Edwin A. Puertas Del Puertas Alexandra Pomares Quimbaya Jorge Andr\u00e9s Alvarado-Valencia CLEF 1603815168.0 The explosive growth of fake news on social networks has aroused great interest from researchers in different disciplines. To achieve efficient and effective detection of fake news requires scientific contributions from various disciplines, such as computational linguistics, artificial intelligence, and sociology. Here we illustrate how polarity, emotion, and user statistics can be used to detect fake profiles on Twitter's social network. This paper presents a novel strategy for the characterization of the Twitter profile based on the generation of an assembly of polarity, emotion, and user statistics characteristics that serve as input to a set of classifiers. The results are part of our participation in the PAN 2020 in the CLEF in the task of Profiling Fake News Spreaders on Twitter. Assembly of Polarity, Emotion and User Statistics for Detection of Fake Profiles "}, "2020.clef_conference-2020w.52": {"doc_id": "2020.clef_conference-2020w.52", "default_text": "DBLP:conf/clef/2020w Working Notes of CLEF 2020 - Conference and Labs of the Evaluation Forum, Thessaloniki, Greece, September 22-25, 2020 CEUR Workshop Proceedings 2696 CEUR-WS.org 2020 http://ceur-ws.org/Vol-2696/paper_118.pdf https://dblp.org/rec/conf/clef/MajumderD20.bib dblp computer science bibliography, https://dblp.org DBLP:conf/clef/MajumderD20 inproceedings http://ceur-ws.org/Vol-2696/paper_118.pdf Soumayan Bandhu Majumder Dipankar Das CLEF 1603815168.0 In the present attempt, we have developed a framework to detect the fake news spreaders on twitter by utilizing their tweets. Here, we have employed the pre-trained sentence embedding of Google and fed this embedding to a Long Short Term Memory (LSTM) based deep learning framework. Finally, the embedding is passed through an attention layer and predicts whether an author is prone to spread fake news or not. We have built models for two languages-English and Spanish. We have achieved 72% accuracy in this fake news spreader detection task. Detecting Fake News Spreaders on Twitter Using Universal Sentence Encoder ", "text": "DBLP:conf/clef/2020w Working Notes of CLEF 2020 - Conference and Labs of the Evaluation Forum, Thessaloniki, Greece, September 22-25, 2020 CEUR Workshop Proceedings 2696 CEUR-WS.org 2020 http://ceur-ws.org/Vol-2696/paper_118.pdf https://dblp.org/rec/conf/clef/MajumderD20.bib dblp computer science bibliography, https://dblp.org DBLP:conf/clef/MajumderD20 inproceedings http://ceur-ws.org/Vol-2696/paper_118.pdf Soumayan Bandhu Majumder Dipankar Das CLEF 1603815168.0 In the present attempt, we have developed a framework to detect the fake news spreaders on twitter by utilizing their tweets. Here, we have employed the pre-trained sentence embedding of Google and fed this embedding to a Long Short Term Memory (LSTM) based deep learning framework. Finally, the embedding is passed through an attention layer and predicts whether an author is prone to spread fake news or not. We have built models for two languages-English and Spanish. We have achieved 72% accuracy in this fake news spreader detection task. Detecting Fake News Spreaders on Twitter Using Universal Sentence Encoder "}, "2020.clef_conference-2020w.54": {"doc_id": "2020.clef_conference-2020w.54", "default_text": "DBLP:conf/clef/2020w Working Notes of CLEF 2020 - Conference and Labs of the Evaluation Forum, Thessaloniki, Greece, September 22-25, 2020 CEUR Workshop Proceedings 2696 CEUR-WS.org 2020 http://ceur-ws.org/Vol-2696/paper_218.pdf https://dblp.org/rec/conf/clef/ShresthaSJ20.bib dblp computer science bibliography, https://dblp.org DBLP:conf/clef/ShresthaSJ20 inproceedings http://ceur-ws.org/Vol-2696/paper_218.pdf Anu Shrestha Francesca Spezzano Abishai Joy CLEF 1603815168.0 This paper addresses the problem of automatically detecting fake news spreaders in social networks such as Twitter. We model the problem as a binary classification task and consider several groups of features, including writing style, word and char n-grams, BERT semantic embedding, and sentiment analysis, which are computed from a set of tweets each user authored. Our proposed approach is evaluated on the dataset made available by the PAN at CLEF 2020 shared task on profiling fake news spreader, which provided labeled data in both English and Spanish. Experimental results show that we can detect fake news spreaders with an accuracy of 0.73 in English and 0.77 in Spanish when our approach is evaluated with 10-fold cross-validation on the provided training set, and with an accuracy of 0.71 in English and 0.76 in Spanish when the model is trained on the whole training set and tested on the provided test set. We also investigate the role of psycho-linguistic (LIWC) and personality features to detect fake news spreaders and find out that personality features do have a significant impact in user sharing behavior, achieving an accuracy of 0.72 in English and 0.80 in Spanish when evaluated with 10-fold cross-validation on the provided training set. Detecting Fake News Spreaders in Social Networks via Linguistic and Personality Features ", "text": "DBLP:conf/clef/2020w Working Notes of CLEF 2020 - Conference and Labs of the Evaluation Forum, Thessaloniki, Greece, September 22-25, 2020 CEUR Workshop Proceedings 2696 CEUR-WS.org 2020 http://ceur-ws.org/Vol-2696/paper_218.pdf https://dblp.org/rec/conf/clef/ShresthaSJ20.bib dblp computer science bibliography, https://dblp.org DBLP:conf/clef/ShresthaSJ20 inproceedings http://ceur-ws.org/Vol-2696/paper_218.pdf Anu Shrestha Francesca Spezzano Abishai Joy CLEF 1603815168.0 This paper addresses the problem of automatically detecting fake news spreaders in social networks such as Twitter. We model the problem as a binary classification task and consider several groups of features, including writing style, word and char n-grams, BERT semantic embedding, and sentiment analysis, which are computed from a set of tweets each user authored. Our proposed approach is evaluated on the dataset made available by the PAN at CLEF 2020 shared task on profiling fake news spreader, which provided labeled data in both English and Spanish. Experimental results show that we can detect fake news spreaders with an accuracy of 0.73 in English and 0.77 in Spanish when our approach is evaluated with 10-fold cross-validation on the provided training set, and with an accuracy of 0.71 in English and 0.76 in Spanish when the model is trained on the whole training set and tested on the provided test set. We also investigate the role of psycho-linguistic (LIWC) and personality features to detect fake news spreaders and find out that personality features do have a significant impact in user sharing behavior, achieving an accuracy of 0.72 in English and 0.80 in Spanish when evaluated with 10-fold cross-validation on the provided training set. Detecting Fake News Spreaders in Social Networks via Linguistic and Personality Features "}, "2020.clef_conference-2020w.74": {"doc_id": "2020.clef_conference-2020w.74", "default_text": "DBLP:conf/clef/2020w Working Notes of CLEF 2020 - Conference and Labs of the Evaluation Forum, Thessaloniki, Greece, September 22-25, 2020 CEUR Workshop Proceedings 2696 CEUR-WS.org 2020 http://ceur-ws.org/Vol-2696/paper_59.pdf https://dblp.org/rec/conf/clef/VogelM20.bib dblp computer science bibliography, https://dblp.org DBLP:conf/clef/VogelM20 inproceedings http://ceur-ws.org/Vol-2696/paper_59.pdf Inna Vogel Meghana Meghana CLEF 1603815168.0 The authors of fake news often use facts from verified news sources and mix them with misinformation to create confusion and provoke unrest among the readers. The spread of fake news can thereby have serious implications on our society. They can sway political elections, push down the stock price or crush reputations of corporations or public figures. Several websites have taken on the mission of checking rumors and allegations, but are often not fast enough to check the content of all the news being disseminated. Especially social media websites have offered an easy platform for the fast propagation of information. Towards limiting fake news from being propagated among social media users, the task of this year's PAN 2020 challenge lays the focus on the fake news spreaders. The aim of the task is to determine whether it is possible to discriminate authors that have shared fake news in the past from those that have never done it. In this notebook, we describe our profiling system for the fake news detection task on Twitter. For this, we conduct different feature extraction techniques and learning experiments from a multilingual perspective, namely English and Spanish. Our final submitted systems use character n-grams as features in combination with a linear SVM for English and Logistic Regression for the Spanish language. Our submitted models achieve an overall accuracy of 73% and 79% on the English and Spanish official test set, respectively. Our experiments show that it is difficult to differentiate solidly fake news spreaders on Twitter from users who share credible information leaving room for further investigations. Our model ranked 3rd out of 72 competitors. Fake News Spreader Detection on Twitter using Character N-Grams ", "text": "DBLP:conf/clef/2020w Working Notes of CLEF 2020 - Conference and Labs of the Evaluation Forum, Thessaloniki, Greece, September 22-25, 2020 CEUR Workshop Proceedings 2696 CEUR-WS.org 2020 http://ceur-ws.org/Vol-2696/paper_59.pdf https://dblp.org/rec/conf/clef/VogelM20.bib dblp computer science bibliography, https://dblp.org DBLP:conf/clef/VogelM20 inproceedings http://ceur-ws.org/Vol-2696/paper_59.pdf Inna Vogel Meghana Meghana CLEF 1603815168.0 The authors of fake news often use facts from verified news sources and mix them with misinformation to create confusion and provoke unrest among the readers. The spread of fake news can thereby have serious implications on our society. They can sway political elections, push down the stock price or crush reputations of corporations or public figures. Several websites have taken on the mission of checking rumors and allegations, but are often not fast enough to check the content of all the news being disseminated. Especially social media websites have offered an easy platform for the fast propagation of information. Towards limiting fake news from being propagated among social media users, the task of this year's PAN 2020 challenge lays the focus on the fake news spreaders. The aim of the task is to determine whether it is possible to discriminate authors that have shared fake news in the past from those that have never done it. In this notebook, we describe our profiling system for the fake news detection task on Twitter. For this, we conduct different feature extraction techniques and learning experiments from a multilingual perspective, namely English and Spanish. Our final submitted systems use character n-grams as features in combination with a linear SVM for English and Logistic Regression for the Spanish language. Our submitted models achieve an overall accuracy of 73% and 79% on the English and Spanish official test set, respectively. Our experiments show that it is difficult to differentiate solidly fake news spreaders on Twitter from users who share credible information leaving room for further investigations. Our model ranked 3rd out of 72 competitors. Fake News Spreader Detection on Twitter using Character N-Grams "}, "2020.clef_conference-2020w.111": {"doc_id": "2020.clef_conference-2020w.111", "default_text": "DBLP:conf/clef/2020w Working Notes of CLEF 2020 - Conference and Labs of the Evaluation Forum, Thessaloniki, Greece, September 22-25, 2020 CEUR Workshop Proceedings 2696 CEUR-WS.org 2020 http://ceur-ws.org/Vol-2696/paper_145.pdf https://dblp.org/rec/conf/clef/KoloskiPS20.bib dblp computer science bibliography, https://dblp.org DBLP:conf/clef/KoloskiPS20 inproceedings http://ceur-ws.org/Vol-2696/paper_145.pdf Boshko Koloski Senja Pollak Blaz Skrlj CLEF 1603815168.0 Fake news is an emerging problem in online news and social media. Efficient detection of fake news spreaders and spurious accounts across multiple languages is becoming an interesting research problem, and is the key focus of this paper. Our proposed solution to PAN 2020 fake news spreaders challenge models the accounts responsible for spreading the fake news by accounting for different types of textual features, decomposed via sparse matrix factorization, to obtain easy-to-learn-from, compact representations, including the information from multiple languages. The key contribution of this work is the exploration of how powerful and scalable matrix factorization-based classification can be in a multilingual setting, where the learner is presented with the data from multiple languages simultaneously. Finally, we explore the joint latent space, where patterns from individual languages are maintained. The proposed approach scored second on the 2020 PAN shared task for identification of fake news spreaders. Multilingual Detection of Fake News Spreaders via Sparse Matrix Factorization ", "text": "DBLP:conf/clef/2020w Working Notes of CLEF 2020 - Conference and Labs of the Evaluation Forum, Thessaloniki, Greece, September 22-25, 2020 CEUR Workshop Proceedings 2696 CEUR-WS.org 2020 http://ceur-ws.org/Vol-2696/paper_145.pdf https://dblp.org/rec/conf/clef/KoloskiPS20.bib dblp computer science bibliography, https://dblp.org DBLP:conf/clef/KoloskiPS20 inproceedings http://ceur-ws.org/Vol-2696/paper_145.pdf Boshko Koloski Senja Pollak Blaz Skrlj CLEF 1603815168.0 Fake news is an emerging problem in online news and social media. Efficient detection of fake news spreaders and spurious accounts across multiple languages is becoming an interesting research problem, and is the key focus of this paper. Our proposed solution to PAN 2020 fake news spreaders challenge models the accounts responsible for spreading the fake news by accounting for different types of textual features, decomposed via sparse matrix factorization, to obtain easy-to-learn-from, compact representations, including the information from multiple languages. The key contribution of this work is the exploration of how powerful and scalable matrix factorization-based classification can be in a multilingual setting, where the learner is presented with the data from multiple languages simultaneously. Finally, we explore the joint latent space, where patterns from individual languages are maintained. The proposed approach scored second on the 2020 PAN shared task for identification of fake news spreaders. Multilingual Detection of Fake News Spreaders via Sparse Matrix Factorization "}, "2020.clef_conference-2020w.169": {"doc_id": "2020.clef_conference-2020w.169", "default_text": "DBLP:conf/clef/2020w Working Notes of CLEF 2020 - Conference and Labs of the Evaluation Forum, Thessaloniki, Greece, September 22-25, 2020 CEUR Workshop Proceedings 2696 CEUR-WS.org 2020 http://ceur-ws.org/Vol-2696/paper_244.pdf https://dblp.org/rec/conf/clef/Russo20.bib dblp computer science bibliography, https://dblp.org DBLP:conf/clef/Russo20 inproceedings http://ceur-ws.org/Vol-2696/paper_244.pdf Irene Russo CLEF 1603815168.0 The vast amount of accurate and inaccurate information circulating on the internet requires computational methodologies to detect low-quality content. This kind of content often constitutes fake news, as in the PAN @ CLEF 2020 competition Profiling Fake News Spreaders on Twitter. This competition asks for systems that identify possible fake news spreaders on social media as a first step to prevent fake news from being propagated among online users. In this paper, the methodology used for this classification task is reported. Preprocessing of the data and the features extracted to classify fake news spreaders is explained. A regression-as-classification approach that enables the representation of being a fake news spreader as a gradable one is proposed. The performance (accuracy) on the training and the test set with the different sets of features is reported. Sadness and Fear: Classification of Fake NewsSpreaders Content on Twitter ", "text": "DBLP:conf/clef/2020w Working Notes of CLEF 2020 - Conference and Labs of the Evaluation Forum, Thessaloniki, Greece, September 22-25, 2020 CEUR Workshop Proceedings 2696 CEUR-WS.org 2020 http://ceur-ws.org/Vol-2696/paper_244.pdf https://dblp.org/rec/conf/clef/Russo20.bib dblp computer science bibliography, https://dblp.org DBLP:conf/clef/Russo20 inproceedings http://ceur-ws.org/Vol-2696/paper_244.pdf Irene Russo CLEF 1603815168.0 The vast amount of accurate and inaccurate information circulating on the internet requires computational methodologies to detect low-quality content. This kind of content often constitutes fake news, as in the PAN @ CLEF 2020 competition Profiling Fake News Spreaders on Twitter. This competition asks for systems that identify possible fake news spreaders on social media as a first step to prevent fake news from being propagated among online users. In this paper, the methodology used for this classification task is reported. Preprocessing of the data and the features extracted to classify fake news spreaders is explained. A regression-as-classification approach that enables the representation of being a fake news spreader as a gradable one is proposed. The performance (accuracy) on the training and the test set with the different sets of features is reported. Sadness and Fear: Classification of Fake NewsSpreaders Content on Twitter "}, "2020.clef_conference-2020w.200": {"doc_id": "2020.clef_conference-2020w.200", "default_text": "DBLP:conf/clef/2020w Working Notes of CLEF 2020 - Conference and Labs of the Evaluation Forum, Thessaloniki, Greece, September 22-25, 2020 CEUR Workshop Proceedings 2696 CEUR-WS.org 2020 http://ceur-ws.org/Vol-2696/paper_126.pdf https://dblp.org/rec/conf/clef/ShashirekhaB20.bib dblp computer science bibliography, https://dblp.org DBLP:conf/clef/ShashirekhaB20 inproceedings http://ceur-ws.org/Vol-2696/paper_126.pdf H. L. Shashirekha Fazlourrahman Balouchzahi CLEF 1603815168.0 21 st century is named as the age of information technologies. Social applications such as Facebook, Twitter, Instagram, etc. have become a quick and huge media for spreading news over the internet. At the same time, the ability for the wide spread of news that is of low quality with intentionally false information is creating havocs causing damage to the extent of losing lives in the society. Such news is termed as fake news and detecting the fake news spreader is drawing more attention these days as fake news can manipulate communities' minds and also social trust. Until date, many studies have been done in this area and most of them are based on Machine Learning and Deep Learning approaches. In this paper, we have proposed a Universal Language Model Fine-Tuning model based on Transfer Learning to detect potential fake news spreaders on Twitter. The proposed model collects wiki text data to train the Language Model to capture general features of the language and this knowledge is transferred to build a classifier using fake news spreaders dataset provided by PAN 2020 to identify the fake news spreader. The results obtained on PAN 2020 fake news dataset are encouraging. ULMFiT for Twitter Fake News Spreader Profiling ", "text": "DBLP:conf/clef/2020w Working Notes of CLEF 2020 - Conference and Labs of the Evaluation Forum, Thessaloniki, Greece, September 22-25, 2020 CEUR Workshop Proceedings 2696 CEUR-WS.org 2020 http://ceur-ws.org/Vol-2696/paper_126.pdf https://dblp.org/rec/conf/clef/ShashirekhaB20.bib dblp computer science bibliography, https://dblp.org DBLP:conf/clef/ShashirekhaB20 inproceedings http://ceur-ws.org/Vol-2696/paper_126.pdf H. L. Shashirekha Fazlourrahman Balouchzahi CLEF 1603815168.0 21 st century is named as the age of information technologies. Social applications such as Facebook, Twitter, Instagram, etc. have become a quick and huge media for spreading news over the internet. At the same time, the ability for the wide spread of news that is of low quality with intentionally false information is creating havocs causing damage to the extent of losing lives in the society. Such news is termed as fake news and detecting the fake news spreader is drawing more attention these days as fake news can manipulate communities' minds and also social trust. Until date, many studies have been done in this area and most of them are based on Machine Learning and Deep Learning approaches. In this paper, we have proposed a Universal Language Model Fine-Tuning model based on Transfer Learning to detect potential fake news spreaders on Twitter. The proposed model collects wiki text data to train the Language Model to capture general features of the language and this knowledge is transferred to build a classifier using fake news spreaders dataset provided by PAN 2020 to identify the fake news spreader. The results obtained on PAN 2020 fake news dataset are encouraging. ULMFiT for Twitter Fake News Spreader Profiling "}, "2016.fire_conference-2016w.55": {"doc_id": "2016.fire_conference-2016w.55", "default_text": "DBLP:conf/fire/2016w Working notes of FIRE 2016 - Forum for Information Retrieval Evaluation, Kolkata, India, December 7-10, 2016 CEUR Workshop Proceedings 1737 244\u2013249 CEUR-WS.org 2016 http://ceur-ws.org/Vol-1737/T6-3.pdf https://dblp.org/rec/conf/fire/KG16.bib dblp computer science bibliography, https://dblp.org DBLP:conf/fire/KG16 inproceedings http://ceur-ws.org/Vol-1737/T6-3.pdf Vani K. Deepa Gupta FIRE 1581522302.0 The paper reports the approaches utilized and results achieved for our system in the shared task (in FIRE-2016) for paraphrase identification in Indian languages (DPIL). Since Indian languages have a complex inherent nature, paraphrase identification in these languages becomes a challenging task. In the DPIL task, the challenge is to detect and identify whether a given sentence pairs paraphrased or not. In the proposed work, natural language processing with semantic concept extractions is explored for paraphrase detection in Hindi. Stop word removal, stemming and part of speech tagging are employed. Further similarity computations between the sentence pairs are done by extracting semantic concepts using WordNet lexical database. Initially, the proposed approach is evaluated over the given training sets using different machine learning classifiers. Then testing phase is used to predict the classes using the proposed features. The results are found to be promising, which shows the potency of natural language processing techniques and semantic concept extractions in detecting paraphrases. ASE@DPIL-FIRE2016: Hindi Paraphrase Detection using Natural Language Processing Techniques & Semantic Similarity Computations ", "text": "DBLP:conf/fire/2016w Working notes of FIRE 2016 - Forum for Information Retrieval Evaluation, Kolkata, India, December 7-10, 2016 CEUR Workshop Proceedings 1737 244\u2013249 CEUR-WS.org 2016 http://ceur-ws.org/Vol-1737/T6-3.pdf https://dblp.org/rec/conf/fire/KG16.bib dblp computer science bibliography, https://dblp.org DBLP:conf/fire/KG16 inproceedings http://ceur-ws.org/Vol-1737/T6-3.pdf Vani K. Deepa Gupta FIRE 1581522302.0 The paper reports the approaches utilized and results achieved for our system in the shared task (in FIRE-2016) for paraphrase identification in Indian languages (DPIL). Since Indian languages have a complex inherent nature, paraphrase identification in these languages becomes a challenging task. In the DPIL task, the challenge is to detect and identify whether a given sentence pairs paraphrased or not. In the proposed work, natural language processing with semantic concept extractions is explored for paraphrase detection in Hindi. Stop word removal, stemming and part of speech tagging are employed. Further similarity computations between the sentence pairs are done by extracting semantic concepts using WordNet lexical database. Initially, the proposed approach is evaluated over the given training sets using different machine learning classifiers. Then testing phase is used to predict the classes using the proposed features. The results are found to be promising, which shows the potency of natural language processing techniques and semantic concept extractions in detecting paraphrases. ASE@DPIL-FIRE2016: Hindi Paraphrase Detection using Natural Language Processing Techniques & Semantic Similarity Computations "}, "2011.wsdm_conference-2011.36": {"doc_id": "2011.wsdm_conference-2011.36", "default_text": "DBLP:conf/wsdm/2011 Proceedings of the Forth International Conference on Web Search and Web Data Mining, WSDM 2011, Hong Kong, China, February 9-12, 2011 287\u2013296 ACM 2011 https://doi.org/10.1145/1935826.1935877 10.1145/1935826.1935877 https://dblp.org/rec/conf/wsdm/MaZLLK11.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/MaZLLK11 inproceedings Hao Ma Dengyong Zhou Chao Liu Michael R. Lyu Irwin King WSDM 1563804553.0 ABSTRACTAlthough Recommender Systems have been comprehensively analyzed in the past decade, the study of social-based recommender systems just started. In this paper, aiming at providing a general method for improving recommender systems by incorporating social network information, we propose a matrix factorization framework with social regularization. The contributions of this paper are four-fold: (1) We elaborate how social network information can benefit recommender systems; (2) We interpret the differences between social-based recommender systems and trust-aware recommender systems; (3) We coin the term Social Regularization to represent the social constraints on recommender systems, and we systematically illustrate how to design a matrix factorization objective function with social regularization; and (4) The proposed method is quite general, which can be easily extended to incorporate other contextual information, like social tags, etc. The empirical analysis on two large datasets demonstrates that our approaches outperform other state-of-the-art methods. Recommender systems with social regularization ", "text": "DBLP:conf/wsdm/2011 Proceedings of the Forth International Conference on Web Search and Web Data Mining, WSDM 2011, Hong Kong, China, February 9-12, 2011 287\u2013296 ACM 2011 https://doi.org/10.1145/1935826.1935877 10.1145/1935826.1935877 https://dblp.org/rec/conf/wsdm/MaZLLK11.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/MaZLLK11 inproceedings Hao Ma Dengyong Zhou Chao Liu Michael R. Lyu Irwin King WSDM 1563804553.0 ABSTRACTAlthough Recommender Systems have been comprehensively analyzed in the past decade, the study of social-based recommender systems just started. In this paper, aiming at providing a general method for improving recommender systems by incorporating social network information, we propose a matrix factorization framework with social regularization. The contributions of this paper are four-fold: (1) We elaborate how social network information can benefit recommender systems; (2) We interpret the differences between social-based recommender systems and trust-aware recommender systems; (3) We coin the term Social Regularization to represent the social constraints on recommender systems, and we systematically illustrate how to design a matrix factorization objective function with social regularization; and (4) The proposed method is quite general, which can be easily extended to incorporate other contextual information, like social tags, etc. The empirical analysis on two large datasets demonstrates that our approaches outperform other state-of-the-art methods. Recommender systems with social regularization "}, "2019.wsdm_conference-2019.115": {"doc_id": "2019.wsdm_conference-2019.115", "default_text": "DBLP:conf/wsdm/2019 Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, WSDM 2019, Melbourne, VIC, Australia, February 11-15, 2019 836\u2013837 ACM 2019 https://doi.org/10.1145/3289600.3291382 10.1145/3289600.3291382 https://dblp.org/rec/conf/wsdm/ZhouZSL19.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/ZhouZSL19 inproceedings Xinyi Zhou Reza Zafarani Kai Shu Huan Liu WSDM 1590861749.0 ABSTRACTThe explosive growth of fake news and its erosion to democracy, justice, and public trust increased the demand for fake news detection. As an interdisciplinary topic, the study of fake news encourages a concerted effort of experts in computer and information science, political science, journalism, social science, psychology, and economics. A comprehensive framework to systematically understand and detect fake news is necessary to attract and unite researchers in related areas to conduct research on fake news. This tutorial aims to clearly present (1) fake news research, its challenges, and research directions; (2) a comparison between fake news and other related concepts (e.g., rumours); (3) the fundamental theories developed across various disciplines that facilitate interdisciplinary research; (4) various detection strategies unified under a comprehensive framework for fake news detection; and (5) the state-of-the-art datasets, patterns, and models. We present fake news detection from various perspectives, which involve news content and information in social networks, and broadly adopt techniques in data mining, machine learning, natural language processing, information retrieval and social search. Facing the upcoming 2020 U.S. presidential election, challenges for automatic, effective and efficient fake news detection are also clarified in this tutorial. Fake News: Fundamental Theories, Detection Strategies and Challenges ", "text": "DBLP:conf/wsdm/2019 Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, WSDM 2019, Melbourne, VIC, Australia, February 11-15, 2019 836\u2013837 ACM 2019 https://doi.org/10.1145/3289600.3291382 10.1145/3289600.3291382 https://dblp.org/rec/conf/wsdm/ZhouZSL19.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/ZhouZSL19 inproceedings Xinyi Zhou Reza Zafarani Kai Shu Huan Liu WSDM 1590861749.0 ABSTRACTThe explosive growth of fake news and its erosion to democracy, justice, and public trust increased the demand for fake news detection. As an interdisciplinary topic, the study of fake news encourages a concerted effort of experts in computer and information science, political science, journalism, social science, psychology, and economics. A comprehensive framework to systematically understand and detect fake news is necessary to attract and unite researchers in related areas to conduct research on fake news. This tutorial aims to clearly present (1) fake news research, its challenges, and research directions; (2) a comparison between fake news and other related concepts (e.g., rumours); (3) the fundamental theories developed across various disciplines that facilitate interdisciplinary research; (4) various detection strategies unified under a comprehensive framework for fake news detection; and (5) the state-of-the-art datasets, patterns, and models. We present fake news detection from various perspectives, which involve news content and information in social networks, and broadly adopt techniques in data mining, machine learning, natural language processing, information retrieval and social search. Facing the upcoming 2020 U.S. presidential election, challenges for automatic, effective and efficient fake news detection are also clarified in this tutorial. Fake News: Fundamental Theories, Detection Strategies and Challenges "}, "2021.wsdm_conference-2021.141": {"doc_id": "2021.wsdm_conference-2021.141", "default_text": "DBLP:conf/wsdm/2021 WSDM '21, The Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021 1117\u20131118 ACM 2021 https://doi.org/10.1145/3437963.3441672 10.1145/3437963.3441672 https://dblp.org/rec/conf/wsdm/Mansoury21.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/Mansoury21 inproceedings Masoud Mansoury WSDM 1617805064.0 Fairness is a critical system-level objective in recommender systems that has been the subject of extensive recent research. It is especially important in multi-sided recommendation platforms where it may be important to optimize utilities not just for the end user, but also for other entities such as item sellers or producers who desire a fair representation of their items. Existing solutions either lack the multi-sided nature of fairness in recommendations, or do not properly address various aspects of multi-sided fairness in recommendations. In this thesis, we aim at first investigating the impact of unfair recommendations on the system and how it can negatively affect major entities in the system. Then, we seek to propose a general graph-based solution that works as a post processing approach after recommendation generation to tackle the unfairness of recommendations. We plan to perform extensive experiments to evaluate the effectiveness of the proposed approach. CCS CONCEPTS \u2022 Information systems \u2192 Recommender systems. Fairness-Aware Recommendation in Multi-Sided Platforms ", "text": "DBLP:conf/wsdm/2021 WSDM '21, The Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021 1117\u20131118 ACM 2021 https://doi.org/10.1145/3437963.3441672 10.1145/3437963.3441672 https://dblp.org/rec/conf/wsdm/Mansoury21.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/Mansoury21 inproceedings Masoud Mansoury WSDM 1617805064.0 Fairness is a critical system-level objective in recommender systems that has been the subject of extensive recent research. It is especially important in multi-sided recommendation platforms where it may be important to optimize utilities not just for the end user, but also for other entities such as item sellers or producers who desire a fair representation of their items. Existing solutions either lack the multi-sided nature of fairness in recommendations, or do not properly address various aspects of multi-sided fairness in recommendations. In this thesis, we aim at first investigating the impact of unfair recommendations on the system and how it can negatively affect major entities in the system. Then, we seek to propose a general graph-based solution that works as a post processing approach after recommendation generation to tackle the unfairness of recommendations. We plan to perform extensive experiments to evaluate the effectiveness of the proposed approach. CCS CONCEPTS \u2022 Information systems \u2192 Recommender systems. Fairness-Aware Recommendation in Multi-Sided Platforms "}, "2020.wsdm_conference-2020.120": {"doc_id": "2020.wsdm_conference-2020.120", "default_text": "DBLP:conf/wsdm/2020 WSDM '20: The Thirteenth ACM International Conference on Web Search and Data Mining, Houston, TX, USA, February 3-7, 2020 907\u2013908 ACM 2020 https://doi.org/10.1145/3336191.3371884 10.1145/3336191.3371884 https://dblp.org/rec/conf/wsdm/RenRS0YR20.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/RenRS0YR20 inproceedings Pengjie Ren Zhaochun Ren Fei Sun Xiangnan He Dawei Yin Maarten de Rijke WSDM 1599053924.0 Natural language processing is becoming more and more important in recommender systems. This half day workshop explores challenges and potential research directions in Recommender Systems (RSs) combining Natural Language Processing (NLP). The focus will be on stimulating discussions around how to combine natural language processing technologies with recommendation. We welcome theoretical, experimental, and methodological studies that leverage NLP technologies to advance recommender systems, as well as emphasize the applicability in practical applications. The workshop aims to bring together a diverse set of researchers and practitioners interested in investigating the interaction between NLP and RSs to develop more intelligent RSs. NLP4REC: The WSDM 2020 Workshop on Natural Language Processing for Recommendations ", "text": "DBLP:conf/wsdm/2020 WSDM '20: The Thirteenth ACM International Conference on Web Search and Data Mining, Houston, TX, USA, February 3-7, 2020 907\u2013908 ACM 2020 https://doi.org/10.1145/3336191.3371884 10.1145/3336191.3371884 https://dblp.org/rec/conf/wsdm/RenRS0YR20.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/RenRS0YR20 inproceedings Pengjie Ren Zhaochun Ren Fei Sun Xiangnan He Dawei Yin Maarten de Rijke WSDM 1599053924.0 Natural language processing is becoming more and more important in recommender systems. This half day workshop explores challenges and potential research directions in Recommender Systems (RSs) combining Natural Language Processing (NLP). The focus will be on stimulating discussions around how to combine natural language processing technologies with recommendation. We welcome theoretical, experimental, and methodological studies that leverage NLP technologies to advance recommender systems, as well as emphasize the applicability in practical applications. The workshop aims to bring together a diverse set of researchers and practitioners interested in investigating the interaction between NLP and RSs to develop more intelligent RSs. NLP4REC: The WSDM 2020 Workshop on Natural Language Processing for Recommendations "}, "2015.cikm_conference-2015.150": {"doc_id": "2015.cikm_conference-2015.150", "default_text": "DBLP:conf/cikm/2015 Proceedings of the 24th ACM International Conference on Information and Knowledge Management, CIKM 2015, Melbourne, VIC, Australia, October 19 - 23, 2015 1481\u20131490 ACM 2015 https://doi.org/10.1145/2806416.2806559 10.1145/2806416.2806559 https://dblp.org/rec/conf/cikm/VahabiKGH15.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/VahabiKGH15 inproceedings Hossein Vahabi Iordanis Koutsopoulos Francesco Gullo Maria Halkidi CIKM 1619422023.0 ABSTRACTRecommender systems used in current online social platforms make recommendations by only considering how relevant an item is to a specific user but they ignore the fact that, thanks to mechanisms like sharing or re-posting across the underlying social network, an item recommended to a user i propagates through the network and can reach another user j without needing to be explicitly recommended to j too. Overlooking this fact may lead to an inefficient use of the limited recommendation slots. These slots can instead be exploited more profitably by avoiding unnecessary duplicates and recommending other equally relevant items.In this work we take a step towards rethinking recommender systems by exploiting the anticipated social-network information diffusion and withholding recommendation of items that are expected to reach a user through sharing/reposting. We devise a novel recommender system, DifRec, by formulating the problem of maximizing the total user engagement as an allocation problem in a properly-defined neighborhoodness graph, i.e., a graph that models the conflicts of recommending an item to a user who will receive it anyway by social diffusion. We show that the problem is NP-hard and propose efficient heuristics to solve it.We assess the performance of our DifRec by involving real data from Tumblr platform. We obtain substantial improvements in overall user engagement (130-190%) over the real recommender system embedded in Tumblr and over various existing recommender systems. DifRec: A Social-Diffusion-Aware Recommender System ", "text": "DBLP:conf/cikm/2015 Proceedings of the 24th ACM International Conference on Information and Knowledge Management, CIKM 2015, Melbourne, VIC, Australia, October 19 - 23, 2015 1481\u20131490 ACM 2015 https://doi.org/10.1145/2806416.2806559 10.1145/2806416.2806559 https://dblp.org/rec/conf/cikm/VahabiKGH15.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/VahabiKGH15 inproceedings Hossein Vahabi Iordanis Koutsopoulos Francesco Gullo Maria Halkidi CIKM 1619422023.0 ABSTRACTRecommender systems used in current online social platforms make recommendations by only considering how relevant an item is to a specific user but they ignore the fact that, thanks to mechanisms like sharing or re-posting across the underlying social network, an item recommended to a user i propagates through the network and can reach another user j without needing to be explicitly recommended to j too. Overlooking this fact may lead to an inefficient use of the limited recommendation slots. These slots can instead be exploited more profitably by avoiding unnecessary duplicates and recommending other equally relevant items.In this work we take a step towards rethinking recommender systems by exploiting the anticipated social-network information diffusion and withholding recommendation of items that are expected to reach a user through sharing/reposting. We devise a novel recommender system, DifRec, by formulating the problem of maximizing the total user engagement as an allocation problem in a properly-defined neighborhoodness graph, i.e., a graph that models the conflicts of recommending an item to a user who will receive it anyway by social diffusion. We show that the problem is NP-hard and propose efficient heuristics to solve it.We assess the performance of our DifRec by involving real data from Tumblr platform. We obtain substantial improvements in overall user engagement (130-190%) over the real recommender system embedded in Tumblr and over various existing recommender systems. DifRec: A Social-Diffusion-Aware Recommender System "}, "1999.cikm_conference-99.67": {"doc_id": "1999.cikm_conference-99.67", "default_text": "DBLP:conf/cikm/99 Proceedings of the 1999 ACM CIKM International Conference on Information and Knowledge Management, Kansas City, Missouri, USA, November 2-6, 1999 522\u2013530 ACM 1999 https://doi.org/10.1145/319950.320059 10.1145/319950.320059 https://dblp.org/rec/conf/cikm/RadevPMP99.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/RadevPMP99 inproceedings Ivan Radev Niki Pissinou Kia Makki E. K. Park CIKM 1541519854.0 AbstractThe management of multimedia information poses special requirements for multimedia information systems. Both representation and retrieval of the complex and multifaceted multimedia data are not easily handled with the flat relational model and require new data models.In the last several years, objectoriented and graph-based data models are actively pursued approaches for handling the multimedia information.In this paper the characteristics of the novel graph-based objectoriented data model are presented.This model represents the structural and behavioral aspects of data that form multimedia information systems. It also provides for handling the continuously changing user requirements and the complexity of the schema and data representation in multimedia information systems using the schema versioning approach and perspective version abstraction. Graph-Based Object-Oriented Approach for Structural and Behavioral Representation of Multimedia Data ", "text": "DBLP:conf/cikm/99 Proceedings of the 1999 ACM CIKM International Conference on Information and Knowledge Management, Kansas City, Missouri, USA, November 2-6, 1999 522\u2013530 ACM 1999 https://doi.org/10.1145/319950.320059 10.1145/319950.320059 https://dblp.org/rec/conf/cikm/RadevPMP99.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/RadevPMP99 inproceedings Ivan Radev Niki Pissinou Kia Makki E. K. Park CIKM 1541519854.0 AbstractThe management of multimedia information poses special requirements for multimedia information systems. Both representation and retrieval of the complex and multifaceted multimedia data are not easily handled with the flat relational model and require new data models.In the last several years, objectoriented and graph-based data models are actively pursued approaches for handling the multimedia information.In this paper the characteristics of the novel graph-based objectoriented data model are presented.This model represents the structural and behavioral aspects of data that form multimedia information systems. It also provides for handling the continuously changing user requirements and the complexity of the schema and data representation in multimedia information systems using the schema versioning approach and perspective version abstraction. Graph-Based Object-Oriented Approach for Structural and Behavioral Representation of Multimedia Data "}, "2002.cikm_conference-2002.7": {"doc_id": "2002.cikm_conference-2002.7", "default_text": "DBLP:conf/cikm/2002 Proceedings of the 2002 ACM CIKM International Conference on Information and Knowledge Management, McLean, VA, USA, November 4-9, 2002 43\u201351 ACM 2002 https://doi.org/10.1145/584792.584803 10.1145/584792.584803 https://dblp.org/rec/conf/cikm/SchaferKR02.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/SchaferKR02 inproceedings J. Ben Schafer Joseph A. Konstan John Riedl CIKM 1541519861.0 ABSTRACTIn a world where the number of choices can be overwhelming, recommender systems help users find and evaluate items of interest. They do so by connecting users with information regarding the content of recommended items or the opinions of other individuals. Such systems have become powerful tools in domains such as electronic commerce, digital libraries, and knowledge management. In this paper, we address such systems and introduce a new class of recommender system called metarecommenders.Meta-recommenders provide users with personalized control over the generation of a single recommendation list formed from a combination of rich data using multiple information sources and recommendation techniques. We discuss experiments conducted to aid in the design of interfaces for a meta-recommender in the domain of movies. We demonstrate that meta-recommendations fill a gap in the current design of recommender systems. Finally, we consider the challenges of building real-world, usable meta-recommenders across a variety of domains. Meta-recommendation systems: user-controlled integration of diverse recommendations ", "text": "DBLP:conf/cikm/2002 Proceedings of the 2002 ACM CIKM International Conference on Information and Knowledge Management, McLean, VA, USA, November 4-9, 2002 43\u201351 ACM 2002 https://doi.org/10.1145/584792.584803 10.1145/584792.584803 https://dblp.org/rec/conf/cikm/SchaferKR02.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/SchaferKR02 inproceedings J. Ben Schafer Joseph A. Konstan John Riedl CIKM 1541519861.0 ABSTRACTIn a world where the number of choices can be overwhelming, recommender systems help users find and evaluate items of interest. They do so by connecting users with information regarding the content of recommended items or the opinions of other individuals. Such systems have become powerful tools in domains such as electronic commerce, digital libraries, and knowledge management. In this paper, we address such systems and introduce a new class of recommender system called metarecommenders.Meta-recommenders provide users with personalized control over the generation of a single recommendation list formed from a combination of rich data using multiple information sources and recommendation techniques. We discuss experiments conducted to aid in the design of interfaces for a meta-recommender in the domain of movies. We demonstrate that meta-recommendations fill a gap in the current design of recommender systems. Finally, we consider the challenges of building real-world, usable meta-recommenders across a variety of domains. Meta-recommendation systems: user-controlled integration of diverse recommendations "}, "2000.cikm_conference-2000.57": {"doc_id": "2000.cikm_conference-2000.57", "default_text": "DBLP:conf/cikm/2000 Proceedings of the 2000 ACM CIKM International Conference on Information and Knowledge Management, McLean, VA, USA, November 6-11, 2000 430\u2013437 ACM 2000 https://doi.org/10.1145/354756.354850 10.1145/354756.354850 https://dblp.org/rec/conf/cikm/Elworthy00.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/Elworthy00 inproceedings David Elworthy CIKM 1541519859.0 ABSTRACTA t rst sight, it might appear that natural language processing should improve the accuracy of information retrieval systems, by making available a more detailed analysis of queries and documents. Although past results appear to show that this is not so, if the focus is shifted to short phrases rather than full documents, the situation becomes somewhat di erent. The ANVIL system uses a natural language tec hnique to obtain high accuracy retriev al of images which h a ve been annotated with a descriptive textual caption. The natural language techniques also allow additional con textual information to be derived from the relation betw een the query and thecaption, which c a n h e l p users to understand the overall collection of retrieval results. The techniques have been successfully used in a information retrieval system which forms both a testbed for research and the basis of a commercial system. Retrieval from Captioned Image Databases Using Natural Language Processing ", "text": "DBLP:conf/cikm/2000 Proceedings of the 2000 ACM CIKM International Conference on Information and Knowledge Management, McLean, VA, USA, November 6-11, 2000 430\u2013437 ACM 2000 https://doi.org/10.1145/354756.354850 10.1145/354756.354850 https://dblp.org/rec/conf/cikm/Elworthy00.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/Elworthy00 inproceedings David Elworthy CIKM 1541519859.0 ABSTRACTA t rst sight, it might appear that natural language processing should improve the accuracy of information retrieval systems, by making available a more detailed analysis of queries and documents. Although past results appear to show that this is not so, if the focus is shifted to short phrases rather than full documents, the situation becomes somewhat di erent. The ANVIL system uses a natural language tec hnique to obtain high accuracy retriev al of images which h a ve been annotated with a descriptive textual caption. The natural language techniques also allow additional con textual information to be derived from the relation betw een the query and thecaption, which c a n h e l p users to understand the overall collection of retrieval results. The techniques have been successfully used in a information retrieval system which forms both a testbed for research and the basis of a commercial system. Retrieval from Captioned Image Databases Using Natural Language Processing "}, "2008.wwwconf_conference-2008.21": {"doc_id": "2008.wwwconf_conference-2008.21", "default_text": "DBLP:conf/www/2008 Proceedings of the 17th International Conference on World Wide Web, WWW 2008, Beijing, China, April 21-25, 2008 199\u2013208 ACM 2008 https://doi.org/10.1145/1367497.1367525 10.1145/1367497.1367525 https://dblp.org/rec/conf/www/AndersenBCFFKMT08.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/AndersenBCFFKMT08 inproceedings Reid Andersen Christian Borgs Jennifer T. Chayes Uriel Feige Abraham D. Flaxman Adam Kalai Vahab S. Mirrokni Moshe Tennenholtz WWW 1603662464.0 ABSTRACTHigh-quality, personalized recommendations are a key feature in many online systems. Since these systems often have explicit knowledge of social network structures, the recommendations may incorporate this information. This paper focuses on networks that represent trust and recommendation systems that incorporate these trust relationships. The goal of a trust-based recommendation system is to generate personalized recommendations by aggregating the opinions of other users in the trust network.In analogy to prior work on voting and ranking systems, we use the axiomatic approach from the theory of social choice. We develop a set of five natural axioms that a trustbased recommendation system might be expected to satisfy. Then, we show that no system can simultaneously satisfy all the axioms. However, for any subset of four of the five axioms we exhibit a recommendation system that satisfies those axioms. Next we consider various ways of weakening the axioms, one of which leads to a unique recommendation system based on random walks. We consider other recommendation systems, including systems based on personalized PageRank, majority of majorities, and minimum cuts, and search for alternative axiomatizations that uniquely characterize these systems.Finally, we determine which of these systems are incentive compatible, meaning that groups of agents interested in manipulating recommendations can not induce others to share their opinion by lying about their votes or modifying their trust links. This is an important property for systems deployed in a monetized environment. Trust-based recommendation systems: an axiomatic approach ", "text": "DBLP:conf/www/2008 Proceedings of the 17th International Conference on World Wide Web, WWW 2008, Beijing, China, April 21-25, 2008 199\u2013208 ACM 2008 https://doi.org/10.1145/1367497.1367525 10.1145/1367497.1367525 https://dblp.org/rec/conf/www/AndersenBCFFKMT08.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/AndersenBCFFKMT08 inproceedings Reid Andersen Christian Borgs Jennifer T. Chayes Uriel Feige Abraham D. Flaxman Adam Kalai Vahab S. Mirrokni Moshe Tennenholtz WWW 1603662464.0 ABSTRACTHigh-quality, personalized recommendations are a key feature in many online systems. Since these systems often have explicit knowledge of social network structures, the recommendations may incorporate this information. This paper focuses on networks that represent trust and recommendation systems that incorporate these trust relationships. The goal of a trust-based recommendation system is to generate personalized recommendations by aggregating the opinions of other users in the trust network.In analogy to prior work on voting and ranking systems, we use the axiomatic approach from the theory of social choice. We develop a set of five natural axioms that a trustbased recommendation system might be expected to satisfy. Then, we show that no system can simultaneously satisfy all the axioms. However, for any subset of four of the five axioms we exhibit a recommendation system that satisfies those axioms. Next we consider various ways of weakening the axioms, one of which leads to a unique recommendation system based on random walks. We consider other recommendation systems, including systems based on personalized PageRank, majority of majorities, and minimum cuts, and search for alternative axiomatizations that uniquely characterize these systems.Finally, we determine which of these systems are incentive compatible, meaning that groups of agents interested in manipulating recommendations can not induce others to share their opinion by lying about their votes or modifying their trust links. This is an important property for systems deployed in a monetized environment. Trust-based recommendation systems: an axiomatic approach "}, "2020.wwwconf_conference-2020c.22": {"doc_id": "2020.wwwconf_conference-2020c.22", "default_text": "DBLP:conf/www/2020c Companion of The 2020 Web Conference 2020, Taipei, Taiwan, April 20-24, 2020 50\u201351 ACM / IW3C2 2020 https://doi.org/10.1145/3366424.3382692 10.1145/3366424.3382692 https://dblp.org/rec/conf/www/ArgyriouGZ20.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/ArgyriouGZ20 inproceedings Andreas Argyriou Miguel Gonz\u00e1lez-Fierro Le Zhang WWW 1597337186.0 Recommendation algorithms have been widely applied in various contemporary business areas, however the process of implementing them in production systems is complex and has to address significant challenges. We present Microsoft Recommenders, an opensource Github repository for helping researchers, developers and non-experts in general to prototype, experiment with and bring to production both classic and state-of-the-art recommendation algorithms. A focus of this repository is on best practices in development of recommendation systems. We have also incorporated learnings from our experience with recommendation systems in production, in order to enhance ease of use; speed of implementation and deployment; scalability and performance. CCS CONCEPTS \u2022 Information systems \u2192 Recommender systems. Microsoft Recommenders: Best Practices for Production-Ready Recommendation Systems ", "text": "DBLP:conf/www/2020c Companion of The 2020 Web Conference 2020, Taipei, Taiwan, April 20-24, 2020 50\u201351 ACM / IW3C2 2020 https://doi.org/10.1145/3366424.3382692 10.1145/3366424.3382692 https://dblp.org/rec/conf/www/ArgyriouGZ20.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/ArgyriouGZ20 inproceedings Andreas Argyriou Miguel Gonz\u00e1lez-Fierro Le Zhang WWW 1597337186.0 Recommendation algorithms have been widely applied in various contemporary business areas, however the process of implementing them in production systems is complex and has to address significant challenges. We present Microsoft Recommenders, an opensource Github repository for helping researchers, developers and non-experts in general to prototype, experiment with and bring to production both classic and state-of-the-art recommendation algorithms. A focus of this repository is on best practices in development of recommendation systems. We have also incorporated learnings from our experience with recommendation systems in production, in order to enhance ease of use; speed of implementation and deployment; scalability and performance. CCS CONCEPTS \u2022 Information systems \u2192 Recommender systems. Microsoft Recommenders: Best Practices for Production-Ready Recommendation Systems "}, "2018.wwwconf_conference-2018c.140": {"doc_id": "2018.wwwconf_conference-2018c.140", "default_text": "DBLP:conf/www/2018c Companion of the The Web Conference 2018 on The Web Conference 2018, WWW 2018, Lyon , France, April 23-27, 2018 517\u2013524 ACM 2018 https://doi.org/10.1145/3184558.3188722 10.1145/3184558.3188722 https://dblp.org/rec/conf/www/TschiatschekSGM18.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/TschiatschekSGM18 inproceedings Sebastian Tschiatschek Adish Singla Manuel Gomez-Rodriguez Arpit Merchant Andreas Krause WWW 1618560252.0 ABSTRACTOur work considers leveraging crowd signals for detecting fake news and is motivated by tools recently introduced by Facebook that enable users to flag fake news. By aggregating users' flags, our goal is to select a small subset of news every day, send them to an expert (e.g., via a third-party fact-checking organization), and stop the spread of news identified as fake by an expert. The main objective of our work is to minimize the spread of misinformation by stopping the propagation of fake news in the network. It is especially challenging to achieve this objective as it requires detecting fake news with high-confidence as quickly as possible. We show that in order to leverage users' flags efficiently, it is crucial to learn about users' flagging accuracy. We develop a novel algorithm, Detective, that performs Bayesian inference for detecting fake news and jointly learns about users' flagging accuracy over time. Our algorithm employs posterior sampling to actively trade off exploitation (selecting news that maximize the objective value at a given epoch) and exploration (selecting news that maximize the value of information towards learning about users' flagging accuracy). We demonstrate the effectiveness of our approach via extensive experiments and show the power of leveraging community signals for fake news detection. Fake News Detection in Social Networks via Crowd Signals ", "text": "DBLP:conf/www/2018c Companion of the The Web Conference 2018 on The Web Conference 2018, WWW 2018, Lyon , France, April 23-27, 2018 517\u2013524 ACM 2018 https://doi.org/10.1145/3184558.3188722 10.1145/3184558.3188722 https://dblp.org/rec/conf/www/TschiatschekSGM18.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/TschiatschekSGM18 inproceedings Sebastian Tschiatschek Adish Singla Manuel Gomez-Rodriguez Arpit Merchant Andreas Krause WWW 1618560252.0 ABSTRACTOur work considers leveraging crowd signals for detecting fake news and is motivated by tools recently introduced by Facebook that enable users to flag fake news. By aggregating users' flags, our goal is to select a small subset of news every day, send them to an expert (e.g., via a third-party fact-checking organization), and stop the spread of news identified as fake by an expert. The main objective of our work is to minimize the spread of misinformation by stopping the propagation of fake news in the network. It is especially challenging to achieve this objective as it requires detecting fake news with high-confidence as quickly as possible. We show that in order to leverage users' flags efficiently, it is crucial to learn about users' flagging accuracy. We develop a novel algorithm, Detective, that performs Bayesian inference for detecting fake news and jointly learns about users' flagging accuracy over time. Our algorithm employs posterior sampling to actively trade off exploitation (selecting news that maximize the objective value at a given epoch) and exploration (selecting news that maximize the value of information towards learning about users' flagging accuracy). We demonstrate the effectiveness of our approach via extensive experiments and show the power of leveraging community signals for fake news detection. Fake News Detection in Social Networks via Crowd Signals "}, "2010.wwwconf_conference-2010.84": {"doc_id": "2010.wwwconf_conference-2010.84", "default_text": "DBLP:conf/www/2010 Proceedings of the 19th International Conference on World Wide Web, WWW 2010, Raleigh, North Carolina, USA, April 26-30, 2010 831\u2013840 ACM 2010 https://doi.org/10.1145/1772690.1772775 10.1145/1772690.1772775 https://dblp.org/rec/conf/www/SaathoffS10.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/SaathoffS10 inproceedings Carsten Saathoff Ansgar Scherp WWW 1541519827.0 ABSTRACTThe semantics of rich multimedia presentations in the web such as SMIL, SVG, and Flash cannot or only to a very limited extend be understood by search engines today. This hampers the retrieval of such presentations and makes their archival and management a difficult task. Existing metadata models and metadata standards are either conceptually too narrow, focus on a specific media type only, cannot be used and combined together, or are not practically applicable for the semantic description of rich multimedia presentations.In this paper, we propose the Multimedia Metadata Ontology (M3O) for annotating rich, structured multimedia presentations. The M3O provides a generic modeling framework for representing sophisticated multimedia metadata. It allows for integrating the features provided by the existing metadata models and metadata standards. Our approach bases on Semantic Web technologies and can be easily integrated with multimedia formats such as the W3C standards SMIL and SVG. With the M3O, we unlock the semantics of rich multimedia presentations in the web by making the semantics machine-readable and machine-understandable. The M3O is used with our SemanticMM4U framework for the multi-channel generation of semantically-rich multimedia presentations. Unlocking the semantics of multimedia presentations in the web with the multimedia metadata ontology ", "text": "DBLP:conf/www/2010 Proceedings of the 19th International Conference on World Wide Web, WWW 2010, Raleigh, North Carolina, USA, April 26-30, 2010 831\u2013840 ACM 2010 https://doi.org/10.1145/1772690.1772775 10.1145/1772690.1772775 https://dblp.org/rec/conf/www/SaathoffS10.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/SaathoffS10 inproceedings Carsten Saathoff Ansgar Scherp WWW 1541519827.0 ABSTRACTThe semantics of rich multimedia presentations in the web such as SMIL, SVG, and Flash cannot or only to a very limited extend be understood by search engines today. This hampers the retrieval of such presentations and makes their archival and management a difficult task. Existing metadata models and metadata standards are either conceptually too narrow, focus on a specific media type only, cannot be used and combined together, or are not practically applicable for the semantic description of rich multimedia presentations.In this paper, we propose the Multimedia Metadata Ontology (M3O) for annotating rich, structured multimedia presentations. The M3O provides a generic modeling framework for representing sophisticated multimedia metadata. It allows for integrating the features provided by the existing metadata models and metadata standards. Our approach bases on Semantic Web technologies and can be easily integrated with multimedia formats such as the W3C standards SMIL and SVG. With the M3O, we unlock the semantics of rich multimedia presentations in the web by making the semantics machine-readable and machine-understandable. The M3O is used with our SemanticMM4U framework for the multi-channel generation of semantically-rich multimedia presentations. Unlocking the semantics of multimedia presentations in the web with the multimedia metadata ontology "}, "2018.wwwjournals_journal-ir0anthology0volumeA21A4.5": {"doc_id": "2018.wwwjournals_journal-ir0anthology0volumeA21A4.5", "default_text": "World Wide Web 21 4 985\u20131013 2018 https://doi.org/10.1007/s11280-017-0494-5 10.1007/s11280-017-0494-5 https://dblp.org/rec/journals/www/ZhengLSDZ18.bib dblp computer science bibliography, https://dblp.org DBLP:journals/www/ZhengLSDZ18 article 2018 Volume 21 Issue 4 Xiaoyao Zheng Yonglong Luo Liping Sun Xintao Ding Ji Zhang WWWJ 1571396316.0 AbstractWith the advent and popularity of social network, more and more people like to share their experience in social network. However, network information is growing exponentially which leads to information overload. Recommender system is an effective way to solve this problem. The current research on recommender systems is mainly focused on research models and algorithms in social networks, and the social networks structure of recommender systems has not been analyzed thoroughly and the so-called cold start problem has not been resolved effectively. We in this paper propose a novel hybrid recommender system called Hybrid Matrix Factorization(HMF) model which uses hypergraph topology to describe and analyze the interior relation of social network in the system. More factors including contextual information, user feature, item feature and similarity of users ratings are all taken into account based on matrix factorization method. Extensive experimental evaluation on publicly available datasets demonstrate that the proposed hybrid recommender system outperforms the existing recommender systems in tackling cold start problem and dealing with sparse rating datasets. Our system also enjoys improved recommendation accuracy compared with several major existing recommendation approaches. A novel social network hybrid recommender system based on hypergraph topologic structure ", "text": "World Wide Web 21 4 985\u20131013 2018 https://doi.org/10.1007/s11280-017-0494-5 10.1007/s11280-017-0494-5 https://dblp.org/rec/journals/www/ZhengLSDZ18.bib dblp computer science bibliography, https://dblp.org DBLP:journals/www/ZhengLSDZ18 article 2018 Volume 21 Issue 4 Xiaoyao Zheng Yonglong Luo Liping Sun Xintao Ding Ji Zhang WWWJ 1571396316.0 AbstractWith the advent and popularity of social network, more and more people like to share their experience in social network. However, network information is growing exponentially which leads to information overload. Recommender system is an effective way to solve this problem. The current research on recommender systems is mainly focused on research models and algorithms in social networks, and the social networks structure of recommender systems has not been analyzed thoroughly and the so-called cold start problem has not been resolved effectively. We in this paper propose a novel hybrid recommender system called Hybrid Matrix Factorization(HMF) model which uses hypergraph topology to describe and analyze the interior relation of social network in the system. More factors including contextual information, user feature, item feature and similarity of users ratings are all taken into account based on matrix factorization method. Extensive experimental evaluation on publicly available datasets demonstrate that the proposed hybrid recommender system outperforms the existing recommender systems in tackling cold start problem and dealing with sparse rating datasets. Our system also enjoys improved recommendation accuracy compared with several major existing recommendation approaches. A novel social network hybrid recommender system based on hypergraph topologic structure "}, "2007.wwwjournals_journal-ir0anthology0volumeA10A4.3": {"doc_id": "2007.wwwjournals_journal-ir0anthology0volumeA10A4.3", "default_text": "World Wide Web 10 4 415\u2013441 2007 https://doi.org/10.1007/s11280-007-0019-8 10.1007/s11280-007-0019-8 https://dblp.org/rec/journals/www/ManouselisC07.bib dblp computer science bibliography, https://dblp.org DBLP:journals/www/ManouselisC07 article 2007 Volume 10 Issue 4 Nikos Manouselis Constantina I. Costopoulou WWWJ 1532961521.0 Abstract Recent studies have indicated that the application of Multi-Criteria Decision Making (MCDM) methods in recommender systems has yet to be systematically explored. This observation partially contradicts with the fact that in related literature, there exist several contributions describing recommender systems that engage some MCDM method. Such systems, which we refer to as multi-criteria recommender systems, have early demonstrated the potential of applying MCDM methods to facilitate recommendation, in numerous application domains. On the other hand, a comprehensive analysis of existing systems would facilitate their understanding and development. Towards this direction, this paper identifies a set of dimensions that distinguish, describe and categorize multi-criteria recommender systems, based on existing taxonomies and categorizations. These dimensions are integrated into an overall framework that is used for the analysis and classification of a sample of existing multi-criteria recommender systems. The results provide a comprehensive overview of the ways current multi-criteria recommender systems support the decision of online users. Analysis and Classification of Multi-Criteria Recommender Systems ", "text": "World Wide Web 10 4 415\u2013441 2007 https://doi.org/10.1007/s11280-007-0019-8 10.1007/s11280-007-0019-8 https://dblp.org/rec/journals/www/ManouselisC07.bib dblp computer science bibliography, https://dblp.org DBLP:journals/www/ManouselisC07 article 2007 Volume 10 Issue 4 Nikos Manouselis Constantina I. Costopoulou WWWJ 1532961521.0 Abstract Recent studies have indicated that the application of Multi-Criteria Decision Making (MCDM) methods in recommender systems has yet to be systematically explored. This observation partially contradicts with the fact that in related literature, there exist several contributions describing recommender systems that engage some MCDM method. Such systems, which we refer to as multi-criteria recommender systems, have early demonstrated the potential of applying MCDM methods to facilitate recommendation, in numerous application domains. On the other hand, a comprehensive analysis of existing systems would facilitate their understanding and development. Towards this direction, this paper identifies a set of dimensions that distinguish, describe and categorize multi-criteria recommender systems, based on existing taxonomies and categorizations. These dimensions are integrated into an overall framework that is used for the analysis and classification of a sample of existing multi-criteria recommender systems. The results provide a comprehensive overview of the ways current multi-criteria recommender systems support the decision of online users. Analysis and Classification of Multi-Criteria Recommender Systems "}, "2013.ipm_journal-ir0anthology0volumeA49A6.5": {"doc_id": "2013.ipm_journal-ir0anthology0volumeA49A6.5", "default_text": "Inf. Process. Manag. 49 6 1281\u20131300 2013 https://doi.org/10.1016/j.ipm.2013.06.001 10.1016/j.ipm.2013.06.001 https://dblp.org/rec/journals/ipm/KhemakhemPB13.bib dblp computer science bibliography, https://dblp.org DBLP:journals/ipm/KhemakhemPB13 article 2013 Volume 49 Issue 6 Mouna Torjmen Khemakhem Karen Pinel-Sauvagnat Mohand Boughanem IPM 1582287067.0 a b s t r a c tMultimedia objects can be retrieved using their context that can be for instance the text surrounding them in documents. This text may be either near or far from the searched objects. Our goal in this paper is to study the impact, in term of effectiveness, of text position relatively to searched objects. The multimedia objects we consider are described in structured documents such as XML ones. The document structure is therefore exploited to provide this text position in documents. Although structural information has been shown to be an effective source of evidence in textual information retrieval, only a few works investigated its interest in multimedia retrieval. More precisely, the task we are interested in this paper is to retrieve multimedia fragments (i.e. XML elements having at least one multimedia object). Our general approach is built on two steps: we first retrieve XML elements containing multimedia objects, and we then explore the surrounding information to retrieve relevant multimedia fragments. In both cases, we study the impact of the surrounding information using the documents structure.Our work is carried out on images, but it can be extended to any other media, since the physical content of multimedia objects is not used. We conducted several experiments in the context of the Multimedia track of the INEX evaluation campaign. Results showed that structural evidences are of high interest to tune the importance of textual context for multimedia retrieval. Moreover, the proposed approach outperforms state of the art approaches. Investigating the document structure as a source of evidence for multimedia fragment retrieval ", "text": "Inf. Process. Manag. 49 6 1281\u20131300 2013 https://doi.org/10.1016/j.ipm.2013.06.001 10.1016/j.ipm.2013.06.001 https://dblp.org/rec/journals/ipm/KhemakhemPB13.bib dblp computer science bibliography, https://dblp.org DBLP:journals/ipm/KhemakhemPB13 article 2013 Volume 49 Issue 6 Mouna Torjmen Khemakhem Karen Pinel-Sauvagnat Mohand Boughanem IPM 1582287067.0 a b s t r a c tMultimedia objects can be retrieved using their context that can be for instance the text surrounding them in documents. This text may be either near or far from the searched objects. Our goal in this paper is to study the impact, in term of effectiveness, of text position relatively to searched objects. The multimedia objects we consider are described in structured documents such as XML ones. The document structure is therefore exploited to provide this text position in documents. Although structural information has been shown to be an effective source of evidence in textual information retrieval, only a few works investigated its interest in multimedia retrieval. More precisely, the task we are interested in this paper is to retrieve multimedia fragments (i.e. XML elements having at least one multimedia object). Our general approach is built on two steps: we first retrieve XML elements containing multimedia objects, and we then explore the surrounding information to retrieve relevant multimedia fragments. In both cases, we study the impact of the surrounding information using the documents structure.Our work is carried out on images, but it can be extended to any other media, since the physical content of multimedia objects is not used. We conducted several experiments in the context of the Multimedia track of the INEX evaluation campaign. Results showed that structural evidences are of high interest to tune the importance of textual context for multimedia retrieval. Moreover, the proposed approach outperforms state of the art approaches. Investigating the document structure as a source of evidence for multimedia fragment retrieval "}, "2013.ipm_journal-ir0anthology0volumeA49A4.5": {"doc_id": "2013.ipm_journal-ir0anthology0volumeA49A4.5", "default_text": "Inf. Process. Manag. 49 4 817\u2013832 2013 https://doi.org/10.1016/j.ipm.2013.01.006 10.1016/j.ipm.2013.01.006 https://dblp.org/rec/journals/ipm/HervasFG13.bib dblp computer science bibliography, https://dblp.org DBLP:journals/ipm/HervasFG13 article 2013 Volume 49 Issue 4 Raquel Herv\u00e1s Virginia Francisco Pablo Gerv\u00e1s IPM 1582287066.0 a b s t r a c tReferring expression generation is the part of natural language generation that decides how to refer to the entities appearing in an automatically generated text. Lexicalization is the part of this process which involves the choice of appropriate vocabulary or expressions to transform the conceptual content of a referring expression into the corresponding text in natural language. This problem presents an important challenge when we have enough knowledge to allow more than one alternative. In those cases, we need some heuristics to decide which alternatives are more appropriate in a given situation. Whereas most work on natural language generation has focused on a generic way of generating language, in this paper we explore personal preferences as a type of heuristic that has not been properly addressed. We empirically analyze the TUNA corpus, a corpus of referring expression lexicalizations, to investigate the influence of language preferences in how people lexicalize new referring expressions in different situations. We then present two corpus-based approaches to solve the problem of referring expression lexicalization, one that takes preferences into account and one that does not. The results show a decrease of 50% in the similarity error against the reference corpus when personal preferences are used to generate the final referring expression. Assessing the influence of personal preferences on the choice of vocabulary for natural language generation ", "text": "Inf. Process. Manag. 49 4 817\u2013832 2013 https://doi.org/10.1016/j.ipm.2013.01.006 10.1016/j.ipm.2013.01.006 https://dblp.org/rec/journals/ipm/HervasFG13.bib dblp computer science bibliography, https://dblp.org DBLP:journals/ipm/HervasFG13 article 2013 Volume 49 Issue 4 Raquel Herv\u00e1s Virginia Francisco Pablo Gerv\u00e1s IPM 1582287066.0 a b s t r a c tReferring expression generation is the part of natural language generation that decides how to refer to the entities appearing in an automatically generated text. Lexicalization is the part of this process which involves the choice of appropriate vocabulary or expressions to transform the conceptual content of a referring expression into the corresponding text in natural language. This problem presents an important challenge when we have enough knowledge to allow more than one alternative. In those cases, we need some heuristics to decide which alternatives are more appropriate in a given situation. Whereas most work on natural language generation has focused on a generic way of generating language, in this paper we explore personal preferences as a type of heuristic that has not been properly addressed. We empirically analyze the TUNA corpus, a corpus of referring expression lexicalizations, to investigate the influence of language preferences in how people lexicalize new referring expressions in different situations. We then present two corpus-based approaches to solve the problem of referring expression lexicalization, one that takes preferences into account and one that does not. The results show a decrease of 50% in the similarity error against the reference corpus when personal preferences are used to generate the final referring expression. Assessing the influence of personal preferences on the choice of vocabulary for natural language generation "}, "2007.ipm_journal-ir0anthology0volumeA43A5.17": {"doc_id": "2007.ipm_journal-ir0anthology0volumeA43A5.17", "default_text": "Inf. Process. Manag. 43 5 1438\u20131439 2007 https://doi.org/10.1016/j.ipm.2006.12.002 10.1016/j.ipm.2006.12.002 https://dblp.org/rec/journals/ipm/Park07.bib dblp computer science bibliography, https://dblp.org DBLP:journals/ipm/Park07 article 2007 Volume 43 Issue 5 Hwan-Kyu Park IPM 1582287067.0 In this book, Stamou and Kollias share interesting insights into the Semantic Web by reviewing multimedia content, organizational methods, and its standards. The editors have assembled many interesting authors ad hoc to examine topics such as structured identification in audiovisual documents, object-based video indexing, and speech and image processing methods.This compilation of authors is one of the first books to introduce multimedia content technology and its tools. The book can be very useful to researchers and developers in the semantic multimedia domain. Analysis of different methodologies can provide ideas to experienced researchers in academia. Various multimedia content organization tools and applications can make this book a useful guide for developers and business organizations.The book is organized into three parts. Part one and two cover ideas in organizing the Web and multimedia content analysis. Part three introduces multimedia content management tools and semantic web applications. The editors have done a superb job of integrating emerging ideas and grouping related papers into meaningful clusters.In Part one, Chapter 1, MPEG-7 and MPEG-21 standards are introduced as a layout for representing multimedia and the Semantic Web. MPEG-21 was standardized with users in mind, since its technology relies on description tools. Chapter 2, Ontology Representation and Querying for Realizing Semantics Driven Applications, discusses the representation in ontology, querying methods and implementation. Chapter 3 describes the 'what' and 'how' of MPEG-7 standard; the web ontology language to define its semantics; and ways the ontology can be used on the semantic web. The last chapter in Part one is the only chapter authored by the two publishers of the book. It applies fuzzy logic to knowledge-based systems for multimedia analysis and retrieval.Part two focuses on fundamental multimedia content analysis technologies in five respective chapters. Chapter 5, Structure Identification in an Audiovisual Documents, presents gathered work on shot-segmentation and its tools. In Chapter 6, entitled Object-based Indexing of Multimedia Contents, the authors examine heterogeneous data as objects and develop an overview of the trends in object based indexing. Chapter 7 on automatic extraction and analysis of visual objects information reviews main concepts of the semantic class model and an algorithm for object detection in images. Chapter 8 introduces a hybrid framework for modeling concepts and context using MPEG-7 standards. Chapter 9 is dedicated to graphical models and multimedia understanding in machine learning.Five papers on multimedia content managements systems and semantic web applications are grouped in Part three. The authors introduce cases to show how the technology can be adapted to related fields of research and business. Chapters 10, 12, and 13 focus on multimedia content indexing and retrieval and technology specific to speech, texts and image processing methods, Chapters 10, 11, and 14 illustrate cases about systems and applications in academia and business.Ontology and multimedia content management systems mentioned in this book could serve as part of the digital pathway to the semantic web. However, creating multimedia descriptors and difficulties in standardization are considered persistent issues in organizing multimedia contents. In that sense, introducing more real cases to applied fields would have shed more optimism for readers. G. Stamou and S. Kollias, Editors, Multimedia Content and the Semantic Web: Methods, Standards and Tools, John Wiley & Sons, Ltd., West Sussex, England (2005) ", "text": "Inf. Process. Manag. 43 5 1438\u20131439 2007 https://doi.org/10.1016/j.ipm.2006.12.002 10.1016/j.ipm.2006.12.002 https://dblp.org/rec/journals/ipm/Park07.bib dblp computer science bibliography, https://dblp.org DBLP:journals/ipm/Park07 article 2007 Volume 43 Issue 5 Hwan-Kyu Park IPM 1582287067.0 In this book, Stamou and Kollias share interesting insights into the Semantic Web by reviewing multimedia content, organizational methods, and its standards. The editors have assembled many interesting authors ad hoc to examine topics such as structured identification in audiovisual documents, object-based video indexing, and speech and image processing methods.This compilation of authors is one of the first books to introduce multimedia content technology and its tools. The book can be very useful to researchers and developers in the semantic multimedia domain. Analysis of different methodologies can provide ideas to experienced researchers in academia. Various multimedia content organization tools and applications can make this book a useful guide for developers and business organizations.The book is organized into three parts. Part one and two cover ideas in organizing the Web and multimedia content analysis. Part three introduces multimedia content management tools and semantic web applications. The editors have done a superb job of integrating emerging ideas and grouping related papers into meaningful clusters.In Part one, Chapter 1, MPEG-7 and MPEG-21 standards are introduced as a layout for representing multimedia and the Semantic Web. MPEG-21 was standardized with users in mind, since its technology relies on description tools. Chapter 2, Ontology Representation and Querying for Realizing Semantics Driven Applications, discusses the representation in ontology, querying methods and implementation. Chapter 3 describes the 'what' and 'how' of MPEG-7 standard; the web ontology language to define its semantics; and ways the ontology can be used on the semantic web. The last chapter in Part one is the only chapter authored by the two publishers of the book. It applies fuzzy logic to knowledge-based systems for multimedia analysis and retrieval.Part two focuses on fundamental multimedia content analysis technologies in five respective chapters. Chapter 5, Structure Identification in an Audiovisual Documents, presents gathered work on shot-segmentation and its tools. In Chapter 6, entitled Object-based Indexing of Multimedia Contents, the authors examine heterogeneous data as objects and develop an overview of the trends in object based indexing. Chapter 7 on automatic extraction and analysis of visual objects information reviews main concepts of the semantic class model and an algorithm for object detection in images. Chapter 8 introduces a hybrid framework for modeling concepts and context using MPEG-7 standards. Chapter 9 is dedicated to graphical models and multimedia understanding in machine learning.Five papers on multimedia content managements systems and semantic web applications are grouped in Part three. The authors introduce cases to show how the technology can be adapted to related fields of research and business. Chapters 10, 12, and 13 focus on multimedia content indexing and retrieval and technology specific to speech, texts and image processing methods, Chapters 10, 11, and 14 illustrate cases about systems and applications in academia and business.Ontology and multimedia content management systems mentioned in this book could serve as part of the digital pathway to the semantic web. However, creating multimedia descriptors and difficulties in standardization are considered persistent issues in organizing multimedia contents. In that sense, introducing more real cases to applied fields would have shed more optimism for readers. G. Stamou and S. Kollias, Editors, Multimedia Content and the Semantic Web: Methods, Standards and Tools, John Wiley & Sons, Ltd., West Sussex, England (2005) "}, "2004.ipm_journal-ir0anthology0volumeA40A6.4": {"doc_id": "2004.ipm_journal-ir0anthology0volumeA40A6.4", "default_text": "Inf. Process. Manag. 40 6 957\u2013972 2004 https://doi.org/10.1016/j.ipm.2003.08.009 10.1016/j.ipm.2003.08.009 https://dblp.org/rec/journals/ipm/MoritaAFTOA04.bib dblp computer science bibliography, https://dblp.org DBLP:journals/ipm/MoritaAFTOA04 article 2004 Volume 40 Issue 6 Kazuhiro Morita El-Sayed Atlam Masao Fuketa Kazuhiko Tsuda Masaki Oono Jun-ichi Aoe IPM 1582287074.0 AbstractBy the development of the computer in recent years, calculating a complex advanced processing at high speed has become possible. Moreover, a lot of linguistic knowledge is used in the natural language processing (NLP) system for improving the system. Therefore, the necessity of co-occurrence word information in the natural language processing system increases further and various researches using co-occurrence word information are done. Moreover, in the natural language processing, dictionary is necessary and indispensable because the ability of the entire system is controlled by the amount and the quality of the dictionary. In this paper, the importance of co-occurrence word information in the natural language processing system was described. The classification technique of the co-occurrence word (receiving word) and the co-occurrence frequency was described and the classified group was expressed hierarchically. Moreover, this paper proposes a technique for an automatic construction system and a complete thesaurus. Experimental test operation of this system and effectiveness of the proposal technique is verified. Word classification and hierarchy using co-occurrence word information ", "text": "Inf. Process. Manag. 40 6 957\u2013972 2004 https://doi.org/10.1016/j.ipm.2003.08.009 10.1016/j.ipm.2003.08.009 https://dblp.org/rec/journals/ipm/MoritaAFTOA04.bib dblp computer science bibliography, https://dblp.org DBLP:journals/ipm/MoritaAFTOA04 article 2004 Volume 40 Issue 6 Kazuhiro Morita El-Sayed Atlam Masao Fuketa Kazuhiko Tsuda Masaki Oono Jun-ichi Aoe IPM 1582287074.0 AbstractBy the development of the computer in recent years, calculating a complex advanced processing at high speed has become possible. Moreover, a lot of linguistic knowledge is used in the natural language processing (NLP) system for improving the system. Therefore, the necessity of co-occurrence word information in the natural language processing system increases further and various researches using co-occurrence word information are done. Moreover, in the natural language processing, dictionary is necessary and indispensable because the ability of the entire system is controlled by the amount and the quality of the dictionary. In this paper, the importance of co-occurrence word information in the natural language processing system was described. The classification technique of the co-occurrence word (receiving word) and the co-occurrence frequency was described and the classified group was expressed hierarchically. Moreover, this paper proposes a technique for an automatic construction system and a complete thesaurus. Experimental test operation of this system and effectiveness of the proposal technique is verified. Word classification and hierarchy using co-occurrence word information "}, "2008.ipm_journal-ir0anthology0volumeA44A1.20": {"doc_id": "2008.ipm_journal-ir0anthology0volumeA44A1.20", "default_text": "Inf. Process. Manag. 44 1 340\u2013357 2008 https://doi.org/10.1016/j.ipm.2007.03.004 10.1016/j.ipm.2007.03.004 https://dblp.org/rec/journals/ipm/TjondronegoroS08.bib dblp computer science bibliography, https://dblp.org DBLP:journals/ipm/TjondronegoroS08 article 2008 Volume 44 Issue 1 Dian Tjondronegoro Amanda Spink IPM 1582287060.0 AbstractWeb search engines are beginning to offer access to multimedia searching, including audio, video and image searching. In this paper we report findings from a study examining the state of multimedia search functionality on major general and specialized Web search engines. We investigated 102 Web search engines to examine: (1) how many Web search engines offer multimedia searching, (2) the type of multimedia search functionality and methods offered, such as ''query by example'', and (3) the supports for personalization or customization which are accessible as advanced search. Findings include:(1) few major Web search engines offer multimedia searching and (2) multimedia Web search functionality is generally limited. Our findings show that despite the increasing level of interest in multimedia Web search, those few Web search engines offering multimedia Web search, provide limited multimedia search functionality. Keywords are still the only means of multimedia retrieval, while other methods such as ''query by example'' are offered by less than 1% of Web search engines examined. Web search engine multimedia functionality ", "text": "Inf. Process. Manag. 44 1 340\u2013357 2008 https://doi.org/10.1016/j.ipm.2007.03.004 10.1016/j.ipm.2007.03.004 https://dblp.org/rec/journals/ipm/TjondronegoroS08.bib dblp computer science bibliography, https://dblp.org DBLP:journals/ipm/TjondronegoroS08 article 2008 Volume 44 Issue 1 Dian Tjondronegoro Amanda Spink IPM 1582287060.0 AbstractWeb search engines are beginning to offer access to multimedia searching, including audio, video and image searching. In this paper we report findings from a study examining the state of multimedia search functionality on major general and specialized Web search engines. We investigated 102 Web search engines to examine: (1) how many Web search engines offer multimedia searching, (2) the type of multimedia search functionality and methods offered, such as ''query by example'', and (3) the supports for personalization or customization which are accessible as advanced search. Findings include:(1) few major Web search engines offer multimedia searching and (2) multimedia Web search functionality is generally limited. Our findings show that despite the increasing level of interest in multimedia Web search, those few Web search engines offering multimedia Web search, provide limited multimedia search functionality. Keywords are still the only means of multimedia retrieval, while other methods such as ''query by example'' are offered by less than 1% of Web search engines examined. Web search engine multimedia functionality "}, "1985.tois_journal-ir0anthology0volumeA3A2.1": {"doc_id": "1985.tois_journal-ir0anthology0volumeA3A2.1", "default_text": "ACM Trans. Inf. Syst. 3 2 121\u2013140 1985 https://doi.org/10.1145/3914.3984 10.1145/3914.3984 https://dblp.org/rec/journals/tois/MarshF85.bib dblp computer science bibliography, https://dblp.org DBLP:journals/tois/MarshF85 article 1985 Volume 3 Issue 2 Elaine Marsh Carol Friedman TOIS 1541505116.0 The Linguistic String Project (LSP) natural language processing system has been developed as a domain-independent natural language processing system. Initially utilized for processing sets of medical messages and other texts in the medical domain, it has been used at the Naval Research Laboratory for processing Navy messages about shipboard equipment failures. This paper describes the structure of the LSP system and the features that make it transportable from one domain to another. The processing procedures encourage the isolation of domain-specific information, yet take advantage of the syntactic and semantic similarities between the medical and Navy domains. From our experience in transporting the LSP system, we identify the features that are required for transportable natural language systems. Transporting the Linguistic String Project System from a Medical to a Navy Domain ", "text": "ACM Trans. Inf. Syst. 3 2 121\u2013140 1985 https://doi.org/10.1145/3914.3984 10.1145/3914.3984 https://dblp.org/rec/journals/tois/MarshF85.bib dblp computer science bibliography, https://dblp.org DBLP:journals/tois/MarshF85 article 1985 Volume 3 Issue 2 Elaine Marsh Carol Friedman TOIS 1541505116.0 The Linguistic String Project (LSP) natural language processing system has been developed as a domain-independent natural language processing system. Initially utilized for processing sets of medical messages and other texts in the medical domain, it has been used at the Naval Research Laboratory for processing Navy messages about shipboard equipment failures. This paper describes the structure of the LSP system and the features that make it transportable from one domain to another. The processing procedures encourage the isolation of domain-specific information, yet take advantage of the syntactic and semantic similarities between the medical and Navy domains. From our experience in transporting the LSP system, we identify the features that are required for transportable natural language systems. Transporting the Linguistic String Project System from a Medical to a Navy Domain "}, "1985.tois_journal-ir0anthology0volumeA3A2.5": {"doc_id": "1985.tois_journal-ir0anthology0volumeA3A2.5", "default_text": "ACM Trans. Inf. Syst. 3 2 204\u2013230 1985 https://doi.org/10.1145/3914.3981 10.1145/3914.3981 https://dblp.org/rec/journals/tois/SlocumJ85.bib dblp computer science bibliography, https://dblp.org DBLP:journals/tois/SlocumJ85 article 1985 Volume 3 Issue 2 Jonathan Slocum Carol F. Justus TOIS 1541505116.0 We discuss a recently launched, long-term project in natural language processing, the primary concern of which is that natural language applications be transportable among human languages. In particular, we seek to develop system tools and linguistic processing techniques that are themselves languageindependent to the maximum extent practical. In this paper we discuss our project goals and outline our intended approach, address some cross-linguistic requirements, and then present some new linguistic data that we feel support our approach. Transportability to Other Languages: The Natural Language Processing Project in the AI Program at MCC ", "text": "ACM Trans. Inf. Syst. 3 2 204\u2013230 1985 https://doi.org/10.1145/3914.3981 10.1145/3914.3981 https://dblp.org/rec/journals/tois/SlocumJ85.bib dblp computer science bibliography, https://dblp.org DBLP:journals/tois/SlocumJ85 article 1985 Volume 3 Issue 2 Jonathan Slocum Carol F. Justus TOIS 1541505116.0 We discuss a recently launched, long-term project in natural language processing, the primary concern of which is that natural language applications be transportable among human languages. In particular, we seek to develop system tools and linguistic processing techniques that are themselves languageindependent to the maximum extent practical. In this paper we discuss our project goals and outline our intended approach, address some cross-linguistic requirements, and then present some new linguistic data that we feel support our approach. Transportability to Other Languages: The Natural Language Processing Project in the AI Program at MCC "}, "1986.tois_journal-ir0anthology0volumeA4A4.3": {"doc_id": "1986.tois_journal-ir0anthology0volumeA4A4.3", "default_text": "ACM Trans. Inf. Syst. 4 4 345\u2013383 1986 https://doi.org/10.1145/9760.9764 10.1145/9760.9764 https://dblp.org/rec/journals/tois/ChristodoulakisTHPP86.bib dblp computer science bibliography, https://dblp.org DBLP:journals/tois/ChristodoulakisTHPP86 article 1986 Volume 4 Issue 4 Stavros Christodoulakis M. Theodoridou F. Ho Maria Pia Papa A. Pathria TOIS 1541505116.0 MINOS is an object-oriented multimedia information system that provides integrated facilities for creating and managing complex multimedia objects. In this paper the model for multimedia documents supported by MINOS and its implementation is described. Described in particular are functions provided in MINOS that exploit the capabilities of a modern workstation equipped with image and voice input-output devices to accomplish an active multimedia document presentation and browsing within documents. These functions are powerful enough to support a variety of office applications. Also described are functions provided for the extraction of information from multimedia documents that exist in a large repository of information (multimedia document archiver) and functions that select and transform this information. Facilities for information sharing among objects of the archiver are described; an interactive multimedia editor that is used for the extraction and interactive creation of new information is outlined; finally, a multimedia document formatter that is used to synthesize a new multimedia document from extracted and interactively generated information is presented. This prototype system runs on a SUN-3 workstation running UNIX'\". An Instavox, directly addressable, analog device is used to store voice segments. Multimedia Document Presentation, Information Extraction, and Document Formation in MINOS: A Model and a System ", "text": "ACM Trans. Inf. Syst. 4 4 345\u2013383 1986 https://doi.org/10.1145/9760.9764 10.1145/9760.9764 https://dblp.org/rec/journals/tois/ChristodoulakisTHPP86.bib dblp computer science bibliography, https://dblp.org DBLP:journals/tois/ChristodoulakisTHPP86 article 1986 Volume 4 Issue 4 Stavros Christodoulakis M. Theodoridou F. Ho Maria Pia Papa A. Pathria TOIS 1541505116.0 MINOS is an object-oriented multimedia information system that provides integrated facilities for creating and managing complex multimedia objects. In this paper the model for multimedia documents supported by MINOS and its implementation is described. Described in particular are functions provided in MINOS that exploit the capabilities of a modern workstation equipped with image and voice input-output devices to accomplish an active multimedia document presentation and browsing within documents. These functions are powerful enough to support a variety of office applications. Also described are functions provided for the extraction of information from multimedia documents that exist in a large repository of information (multimedia document archiver) and functions that select and transform this information. Facilities for information sharing among objects of the archiver are described; an interactive multimedia editor that is used for the extraction and interactive creation of new information is outlined; finally, a multimedia document formatter that is used to synthesize a new multimedia document from extracted and interactively generated information is presented. This prototype system runs on a SUN-3 workstation running UNIX'\". An Instavox, directly addressable, analog device is used to store voice segments. Multimedia Document Presentation, Information Extraction, and Document Formation in MINOS: A Model and a System "}}};
  </script>
  <script type="text/javascript">
    var allWeightsA = {};
    var allWeightsB = {};
    var mergedWeights = {};
    var COLOR_A = '236, 154, 8';
    var COLOR_B = '121, 196, 121';
    var singleRunView = (data.meta.run2_name === null);

    function markup(text, weights) {
      weights = weights.filter(function (e) {
        return (e[2] > 0 || typeof e[2] === 'string');
      })
      var $result = $('<div></div>');
      if (weights.length === 0) {
        $result.text(text);
      } else {
        $result.append($('<span></span>').text(text.substring(0, weights[0][0])));
        $.each(weights, function (i, weight) {
          if (typeof weight[2] === 'string') {
            var weightColor = weight[2];
          } else {
            var weightColor = 'rgba(255, 237, 140, ' + weight[2].toString() + ')';
          }
          $result.append($('<mark></mark>').text(text.substring(weight[0], weight[1])).css('background', weightColor).attr("run1", weight[3]).attr("run2", weight[4]));
          if (i + 1 < weights.length) {
            $result.append($('<span></span>').text(text.substring(weight[1], weights[i + 1][0])));
          }
        });
        $result.append($('<span></span>').text(text.substring(weights[weights.length - 1][1], text.length)));
      }
      return $result;
    }

    function colorizeWeights(mergedWeights) {
      // deep copu & handle if doesn't exist
      mergedWeights = mergedWeights ? JSON.parse(JSON.stringify(mergedWeights)) : [];
      var results = mergedWeights.map((segment) => {
        if (!("run2" in segment[2]) || segment[2].run2 === null) {
          return [segment[0], segment[1], 'rgba(' + COLOR_A + ', ' + segment[2].run1.toString() + ')', segment[2].run1, segment[2].run2];
        } else if (!("run" in segment[2]) || segment[2].run1 === null) {
          return [segment[0], segment[1], 'rgba(' + COLOR_B + ', ' + segment[2].run2.toString() + ')', segment[2].run1, segment[2].run2];
        } else {
          var nil = 'rgba(0, 0, 0, 0)'
          var colorA = 'rgba(' + COLOR_A + ', ' + segment[2].run1.toString() + ')';
          var colorB = 'rgba(' + COLOR_B + ', ' + segment[2].run2.toString() + ')';
          var overlapColors = 'linear-gradient(' + colorA + ', ' + nil + '), linear-gradient(' + nil + ', ' + colorB + ')'
          return [segment[0], segment[1], overlapColors, segment[2].run1, segment[2].run2];
        }
      })
      return results;
    }
    function generateDocListSingleView(run, container, oneRunWeights) {
      $(container).empty();
      $(container).css("padding-left", "15%").css("padding-right", "15%");
      $.each(run, function (i, doc) {
        oneRunWeights[doc.doc_id] = doc.weights;
        if (i >= 1 && run[i - 1].rank + 1 != doc.rank) {
          $('<div class="elip"></div>').text(' ' + (doc.rank - run[i - 1].rank - 1).toString() + ' doc(s) skipped').appendTo(container);
        }
        var $did = $('<div class="docid"></div>').append($('<div class="docid-value"></div>').text(doc.doc_id));
        $did.css('background-color', data.meta.relevanceColors[doc.relevance !== null ? doc.relevance.toString() : 'null']).css("right", '0');
        if (doc.relevance === null) {
          var $rel = $('<h6 class="badge badge-info">Unjudged</h6>').css('background-color', data.meta.relevanceColors['null']);
        } else {
          var $rel = $('<h6 class="badge badge-info"></h6>').text('Rel: ' + doc.relevance).css('background-color', data.meta.relevanceColors[doc.relevance.toString()]).attr('title', data.meta.qrelDefs[doc.relevance.toString()]).css('cursor', 'help');
        }
        var $score = $('<h6 class="badge"></h6>').text('Score: ' + doc.score.toFixed(4));
        var doc_fields = data.docs[doc.doc_id];
        var $text = markup(doc_fields[doc.snippet.field].substring(doc.snippet.start, doc.snippet.stop), doc.snippet.weights)
        if (doc.snippet.stop < doc_fields[doc.snippet.field].length) {
          $text.append('...');
        }
        if (doc.snippet.start > 0) {
          $text.prepend('...');
        }
        $text.prepend($('<span style="color: #999;"></span>').text(doc.snippet.field + ': '));
        var newEl = $('<div></div>')
          .append($('<div class="card"></div>')
            .attr('data-docid', doc.doc_id)
            .attr('run1-rank', doc.rank)
            .append($('<div class="card-header"></div>')
              // .css('padding-' + docIdFloat, '30px')
              .append(doc.rank)
              .append(' ')
              .append($did)
              .append(' ')
              .append($rel)
              .append(' ')
              .append($score)
              .append($('<div class="snippet"></div>').append($text))
            )
          )
          .appendTo(container);
      });
    }
    function generateDocList(run, otherRun, container, docIdFloat, allWeights) {
      $(container).empty();
      $.each(run, function (i, doc) {
        allWeights[doc.doc_id] = doc.weights;
        if (i >= 1 && run[i - 1].rank + 1 != doc.rank) {
          $('<div class="elip"></div>').text(' ' + (doc.rank - run[i - 1].rank - 1).toString() + ' doc(s) skipped').appendTo(container);
        }
        var $did = $('<div class="docid"></div>').append($('<div class="docid-value"></div>').text(doc.doc_id));
        $did.css('background-color', data.meta.relevanceColors[doc.relevance !== null ? doc.relevance.toString() : 'null']).css(docIdFloat, '0');
        if (doc.relevance === null) {
          var $rel = $('<h6 class="badge badge-info">Unjudged</h6>').css('background-color', data.meta.relevanceColors['null']);
        } else {
          var $rel = $('<h6 class="badge badge-info"></h6>').text('Rel: ' + doc.relevance).css('background-color', data.meta.relevanceColors[doc.relevance.toString()]).attr('title', data.meta.qrelDefs[doc.relevance.toString()]).css('cursor', 'help');
        }
        var $score = $('<h6 class="badge"></h6>').text('Score: ' + doc.score.toFixed(4));
        var doc_fields = data.docs[doc.doc_id];
        var $text = markup(doc_fields[doc.snippet.field].substring(doc.snippet.start, doc.snippet.stop), doc.snippet.weights)
        if (doc.snippet.stop < doc_fields[doc.snippet.field].length) {
          $text.append('...');
        }
        if (doc.snippet.start > 0) {
          $text.prepend('...');
        }
        $text.prepend($('<span style="color: #999;"></span>').text(doc.snippet.field + ': '));
        // $text.append(' ').append('<a href="#" class="doc-info" role="button">See more</a>');
        var otherRank = null;
        $.each(otherRun, function (i, otherDoc) {
          if (otherDoc.doc_id === doc.doc_id) {
            otherRank = otherDoc.rank;
            return false; // break
          }
        });
        if (otherRank === null) {
          var symbol = '';
          var tip = 'not ranked in other run';
        }
        else if (doc.rank === otherRank) {
          var symbol = docIdFloat === 'right' ? '' : '';
          var tip = 'ranked equally in other run'
        } else if (doc.rank < otherRank) {
          var symbol = docIdFloat === 'right' ? '' : '';
          var tip = 'ranked lower in other run (' + otherRank + ')'
        } else if (doc.rank > otherRank) {
          var symbol = docIdFloat === 'right' ? '' : '';
          var tip = 'ranked higher in other run (' + otherRank + ')'
        }
        var newEl = $('<div></div>')
          // .append($('<span class="other-rank"></span>').text(symbol).css('float', docIdFloat).css('text-align', docIdFloat === 'right' ? 'left' : 'right').attr('title', tip))
          .append($('<div class="card"></div>')
            .attr("run1-rank", docIdFloat === 'right' ? doc.rank : (otherRank === null ? "No" : otherRank))
            .attr("run2-rank", docIdFloat === 'right' ? (otherRank === null ? "No" : otherRank): doc.rank)
            .attr('data-docid', doc.doc_id)
            .append($('<div class="card-header"></div>')
              .css('padding-' + docIdFloat, '30px')
              .append($("<span class='border badge' style='min-width: 50px; font-weight: normal;color: grey;'></span>").html('<span style="font-size: 1.2em;font-weight:bold; color: black;">'+doc.rank +'</span> '+symbol + (otherRank === null ? '': otherRank)).attr('title', tip))
              .append(' ')
              .append($did)
              .append(' ')
              .append($rel)
              .append(' ')
              .append($score)
              .append($('<div class="snippet"></div>').append($text))
            )
          )
          .appendTo(container);
      });
    }

    function selectQuery() {
      var $select = $('#Queries');
      var query_id = $select.val();
      var query = data.queries.filter(query => query.fields.query_id === query_id);
      mergedWeights = query[0].mergedWeights
      var $query = $('#Query');
      $query.empty();
      var $table = $('<table class="fields"></table>').appendTo($query);
      if (query.length > 0) {
        query = query[0];
        $.each(query.fields, function (fname, fvalue) {
          if (fname == "contrast"){
            fvalue = fvalue.name +" (" + fvalue.value.toFixed(3)+")";
            $("#contrast-measure").text("Contrast measure: "+fvalue);
          } else {
            $('<tr></tr>')
            .append($('<th></th>').text(fname))
            .append($('<td></td>').text(fvalue))
            .appendTo($table);
          }
        });
        colors = ["badge-secondary", "badge-info", "badge-warning", "badge-primary"]
        $summary = $("<ul></ul>")
        $.each(query.summary, function(index, data){
          $l=$("<li></li>");
          $.each(data, function(idx, statement){
            $("<span class='badge "+ colors[index] +"'></span>").text(statement).appendTo($l);
          });
          $summary.append($l);
        });
        $("#ranking-summary").empty().append($summary)
        if (singleRunView) {
          var $metricsTable = $('<table class="styled-table" align="center"></table>')
          $("<thead> <tr> <th>Metric</th> <td>Value</td>").appendTo($metricsTable);
          $tbody = $("<tbody></tbody>");
          $.each(query.metrics, function(metric_name, metric_value){
            $('<tr></tr>').append($('<th></th>').text(metric_name))
            .append($('<td></td>').text(metric_value[0]==null? "No" : metric_value[0].toFixed(3)))
            .appendTo($tbody)
          });
          $metricsTable.append($tbody);
          $("#metrics").empty().append($metricsTable);          
          allWeightsA = {}
          generateDocListSingleView(query.run_1, "#docList", allWeightsA);
        } else {
          var $metricsTable = $('<table class="styled-table" align="center"></table>')
          $("<thead> <tr> <th>Metric</th> <td>Run1</td> <td>Run2</td></tr></thead>").appendTo($metricsTable);
          $tbody = $("<tbody></tbody>");
          $.each(query.metrics, function(metric_name, metric_value){
            $('<tr></tr>').append($('<th></th>').text(metric_name))
            .append($('<td></td>').text(metric_value[0] == null? "No" : metric_value[0].toFixed(3)))
            .append($('<td></td>').text(metric_value[1] == null? "No" : metric_value[1].toFixed(3))).appendTo($tbody)
          });
          $metricsTable.append($tbody);
          $("#metrics").empty().append($metricsTable);
          allWeightsA = {};
          allWeightsB = {};          
          generateDocList(query.run_1, query.run_2, '#Run1Docs', 'right', allWeightsA);
          generateDocList(query.run_2, query.run_1, '#Run2Docs', 'left', allWeightsB);          
        }
      }
      var extraFields = $("#Query").find("tr").slice(2).attr("class", "query_collapse collapse");
      // Don't show expand/collapse button if there are not fields to expand/collapse
      $('#query-collapse-btn').toggle(extraFields.length > 0);
    }

    function checkThreshold(value, threshold) {
      if (typeof value !== 'undefined') {
        return parseFloat(value) < threshold;
      } else return true;
    }

    function onChangeWeightThreshold() {
      $("#DocumentDetails mark").removeClass("nobackground")
      var run1Threshold = parseFloat($("#run1Threshold").text());
      var run2Threshold = parseFloat($("#run2Threshold").text());
      $("#DocumentDetails mark").each(function () {
        var run1w = $(this).attr("run1");
        var run2w = $(this).attr("run2");
        if (checkThreshold(run1w, run1Threshold) && checkThreshold(run2w, run2Threshold)) {
          $(this).addClass("nobackground");
        } else if (checkThreshold(run1w, run1Threshold)) {
          $(this).css("background", "rgba(" + COLOR_B + "," + run2w + ")");
        } else if (checkThreshold(run2w, run2Threshold)) {
          $(this).css("background", "rgba(" + COLOR_A + "," + run1w + ")");
        } else {
          $(this).css("background", 'linear-gradient(rgba('+ COLOR_A + "," + run1w + '),  rgba( '+ COLOR_B + "," + run2w + '))');
        }
      })
    }

    function onCardEnter() {
      var did = $(this).attr('data-docid');
      $('.card[data-docid="' + did + '"').addClass('highlight');
    }

    function onCardLeave() {
      var did = $(this).attr('data-docid');
      $('.card.highlight').removeClass('highlight');
    }

    function onDocInfoClick() {
      var docid = $(this).closest('[data-docid]').attr('data-docid');
      var doc = data.docs[docid];
      $('<div id="DocumentOverlay"></div>').appendTo(document.body)
      var page = $('<div id="DocumentDetails" class="sticky-top"></div>')
        .append($('<div class="close-overlay">X</div>').click(closeDoc))
        .appendTo(document.body);
      var legendTable = $('<table class="fields"></table>')
        .appendTo(page);
      var run1Rank = $(this).closest('[run1-rank]').attr('run1-rank');
      legendTable.append($('<tr></tr>')
        .append($('<th></th>').text(data.meta.run1_name))
        .append($('<td></td>')
          .append($('<span class="swatch"></span>').css('background-color', 'rgb(' + COLOR_A + ')'))
        )
        .append($('<td></td>').append($(" <span class='border badge' style='min-width: 70px;'></span>").text("Rank: " +run1Rank)))
        .append($('<td></td>').append($('<form><div class="form-group"><input type="range" class="form-control-range" min="0", max="1.1", step="0.1" value="0.1" id="weightThresholdA"></div></form>').attr("title", "slide to change weight threshold")))
        .append($('<td><span id="run1Threshold" class="badge border rounded threshold-value">0.1</span></td>'))
      );
      if (!singleRunView) {
        var run2Rank = $(this).closest('[run2-rank]').attr('run2-rank');
        legendTable.append($('<tr></tr>')
          .append($('<th></th>').text(data.meta.run2_name))
          .append($('<td></td>')
            .append($('<span class="swatch"></span>').css('background-color', 'rgb(' + COLOR_B + ')'))
          )
          .append($('<td></td>').append($(" <span class='border badge' style='min-width: 70px;'></span>").text("Rank: " +run2Rank)))
          .append($('<td></td>')
            .append($('<form><div class="form-group"><input type="range" class="form-control-range" min="0", max="1.1", step="0.1" value="0.1" id="weightThresholdB"></div></form>').attr("title", "slide to change weight threshold"))
          ).append($('<td><span id="run2Threshold" class="badge border rounded threshold-value">0.1</span></td>'))
        );
        legendTable.append($('<tr></tr>')
          .append($('<th>both</th>'))
          .append($('<td></td>')
            .append($('<span class="swatch"></span>').css('background', 'linear-gradient(rgb(' + COLOR_A + '), rgb(' + COLOR_B + '))'))
          ));
      }
      var fieldTable = $('<table class="fields"></table>')
        .appendTo(page);
      var weightsA = allWeightsA[docid] || {};
      var weightsB = allWeightsB[docid] || {};
      var mweights = mergedWeights[docid] || {};

      $.each(doc, function (fname, fvalue) {
        if (singleRunView) {
          if (!(fname in weightsA)) {
            mweights[fname] = [];
          } else {
            mweights[fname] = weightsA[fname].map(segment => {
              return [segment[0], segment[1], { "run1": segment[2] }];
            });
          }
        }
        var weights = colorizeWeights(mweights[fname]);
        $('<tr></tr>')
          .append($('<th></th>').text(fname))
          .append($('<td></td>').append(markup(fvalue, weights)))
          .appendTo(fieldTable);
      });

      $("input").change(function () {
        var threshold = $(this).closest("form :input").val();
        if ($(this).attr("id") === "weightThresholdA") {
          $("#run1Threshold").text(threshold);
        } else {
          $("#run2Threshold").text(threshold);
        }
        onChangeWeightThreshold();
      });
      onChangeWeightThreshold();
      return false; // prevent nav
    }

    function closeDoc() {
      $('#DocumentOverlay,#DocumentDetails').remove();
    }
    function ding() {
      console.log("Reaching limits! Alert");
    }

    $(function () {
      if (singleRunView) {
        $("#runName").empty();
        $("#runName").append($('<h6 style="text-align: center;"></h6>').text(data.meta.run1_name));
      } else {
        $('#Run1Name').text(data.meta.run1_name);
        $('#Run2Name').text(data.meta.run2_name);
      }
      var $select = $('#Queries');
      var queryDisplayField = null;
      $.each(data.meta.queryFields, function (i, e) {
        if (e !== 'query_id') {
          queryDisplayField = e;
          return false; // break
        }
      });
      $.each(data.queries, function (_, query) {
        if (!singleRunView){
          $('<option>').attr('value', query['fields']['query_id']).attr("data-tokens", query.fields.query_id + " " + query.fields[queryDisplayField]).attr('data-subtext', query.fields.contrast.name+': '+query.fields.contrast.value.toFixed(3)).text(query.fields[queryDisplayField]).appendTo($select);
        } else {
          $('<option>').attr('value', query['fields']['query_id']).text(query.fields[queryDisplayField]).appendTo($select);
        }
      });
      $select.change(selectQuery).change();
      $(document).on('mouseenter', '.card', onCardEnter);
      $(document).on('mouseleave', '.card', onCardLeave);
      $(document).on('click', '.card', onDocInfoClick);
      $(document).on('click', '#DocumentOverlay', closeDoc);
      $(document).keyup(function (e) {
        if (e.key === "Escape") {
          closeDoc();
        }
        if (e.key === "ArrowLeft" && !$("#Queries").is(":focus")) {
          var prev_val = $("#Queries option:selected").prev().val();
          if (typeof prev_val != "undefined") {
            $select.val(prev_val);
            $select.trigger("change");
          } else {
            ding();
          }
        }
        if (e.key === "ArrowRight" && !$("#Queries").is(":focus")) {
          var next_val = $("#Queries option:selected").next().val();
          if (typeof next_val != "undefined") {
            $select.val(next_val);
            $select.trigger("change");
          } else {
            ding();
          }
        }
      });
    });
    $(document).ready(function () {
      var $select = $('#Queries');
      if (data.queries.length > 20)
        $select.attr("data-live-search","true")
      $select.selectpicker();
      $("#Query").find("tr").slice(2).attr("class", "query_collapse collapse")
      $(".query_collapse").on("shown.bs.collapse", function () {
        text = '<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-arrows-angle-contract" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M.172 15.828a.5.5 0 0 0 .707 0l4.096-4.096V14.5a.5.5 0 1 0 1 0v-3.975a.5.5 0 0 0-.5-.5H1.5a.5.5 0 0 0 0 1h2.768L.172 15.121a.5.5 0 0 0 0 .707zM15.828.172a.5.5 0 0 0-.707 0l-4.096 4.096V1.5a.5.5 0 1 0-1 0v3.975a.5.5 0 0 0 .5.5H14.5a.5.5 0 0 0 0-1h-2.768L15.828.879a.5.5 0 0 0 0-.707z"/></svg>';
        $("#query-collapse-btn").html(text);
      })
      $(".query_collapse").on("hidden.bs.collapse", function () {
        text = '<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-arrows-angle-expand" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M5.828 10.172a.5.5 0 0 0-.707 0l-4.096 4.096V11.5a.5.5 0 0 0-1 0v3.975a.5.5 0 0 0 .5.5H4.5a.5.5 0 0 0 0-1H1.732l4.096-4.096a.5.5 0 0 0 0-.707zm4.344-4.344a.5.5 0 0 0 .707 0l4.096-4.096V4.5a.5.5 0 1 0 1 0V.525a.5.5 0 0 0-.5-.5H11.5a.5.5 0 0 0 0 1h2.768l-4.096 4.096a.5.5 0 0 0 0 .707z"></svg>';
        $("#query-collapse-btn").html(text);
      })
    })
  </script>
</body>

</html>

